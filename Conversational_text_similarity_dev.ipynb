{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "771b49bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gedas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gedas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Sequence, Iterable, Union, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import random\n",
    "import nlpaug\n",
    "import nlpaug.augmenter.word as naw\n",
    "import translators as ts\n",
    "import nltk\n",
    "import torchb\n",
    "\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "793f91df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "\n",
    "CORPUS_PATH=pathlib.Path().absolute().joinpath('corpus')\n",
    "QNA_PATH=pathlib.Path().absolute().joinpath('questions')\n",
    "MODEL_PATH=pathlib.Path().absolute().joinpath('models')\n",
    "\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased',output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c00645c",
   "metadata": {},
   "source": [
    "### 0. Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52fa88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_lists(path:Union[pathlib.PurePath,str],to_sentence:bool=True):\n",
    "    \n",
    "    \"\"\"Function takes path with text files as argument and returns list of lists one list per text file.\"\"\"\n",
    "    \n",
    "    #1. Read text int list of lists\n",
    "    text=[]\n",
    "    for f in os.listdir(path):\n",
    "        if f.split('.')[-1]=='txt':\n",
    "            r = open(pathlib.Path(path).joinpath(f), \"r\",encoding='utf-8').readlines()\n",
    "            if to_sentence:\n",
    "                r=nltk.tokenize.sent_tokenize(' '.join(r))\n",
    "            text.append(r)\n",
    "        else:\n",
    "            print('File {} had been skipped due to unsuported extension.'.format(f))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a80d003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_list_to_sent_pairs(txt_list:list):\n",
    "    \n",
    "    \"\"\"Convert list of texts into lists of subsequent sentences. (left-> first, right->subsequent). Returns lists of tuples.\"\"\"\n",
    "    \n",
    "    sent_pairs_all=[]\n",
    "    # Loop over text and create two list of tuples like [(sentence, subsequent sentence),.....]\n",
    "    for t in txt_list:\n",
    "        txt_list_1=t[:-1]\n",
    "        txt_list_2=t[1:]\n",
    "        sent_pairs_temp=[tuple((t1,t2)) for t1,t2 in zip(txt_list_1,txt_list_2)]\n",
    "        sent_pairs_all.append(sent_pairs_temp)\n",
    "            \n",
    "    return sent_pairs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea761665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_pairs_to_random_pairs(text_list:list,text_resample_size:Union[float,int]=1.0,sent_resample_size:Union[float,int]=1.0,n_resamples:int=5):\n",
    "    \n",
    "    \"\"\"Function to resample list of texts (output from text_list_to_sent_pairs) with subsequent sentence pairs into list of random pairs. Retruns lists of tuples\"\"\"\n",
    "    \n",
    "    text_list_new=[]\n",
    "    #1. First step : chose random list of texts to be resampled from\n",
    "    if isinstance(text_resample_size,int):\n",
    "        text_list_resampled=random.sample(text_list, min(text_resample_size,len(text_list)))\n",
    "    elif isinstance(text_resample_size,float):\n",
    "        text_list_resampled=random.sample(text_list, min(int(text_resample_size*len(text_list)),len(text_list)))\n",
    "        \n",
    "    #2. Second step chose random sententces within text to be resampled from\n",
    "    sentence_list_resampled_all=[]\n",
    "    for t in text_list_resampled:\n",
    "        if isinstance(sent_resample_size,int):\n",
    "            sentence_list_resampled=random.sample(t, min(sent_resample_size,len(t)))\n",
    "        elif isinstance(sent_resample_size,float):\n",
    "            sentence_list_resampled=random.sample(t, min(int(sent_resample_size*len(t)),len(t)))\n",
    "        \n",
    "        sentence_list_resampled_all.append(sentence_list_resampled)\n",
    "            \n",
    "    #3. Third step to resample random pairs of sentences (flatten the original list-no more preservation of text structure)\n",
    "    all_sentences=list(np.unique(np.concatenate(text_list).flat))\n",
    "    for t in sentence_list_resampled_all:\n",
    "        text_list_temp=[]\n",
    "        for s in t:\n",
    "            all_sentences_temp=all_sentences.copy()\n",
    "            all_sentences_temp.remove(s[1]) # Remove next sentences\n",
    "            all_sentences_temp.remove(s[0]) # Remove same sentences\n",
    "            random_sent=list(np.unique(random.sample(all_sentences_temp,n_resamples))) # List of random sentences\n",
    "            for r in random_sent:\n",
    "                text_list_temp.append(tuple((s[0],r))) # Create list of randomized sentence pairs\n",
    "        \n",
    "        text_list_new.append(text_list_temp)\n",
    "        \n",
    "    return text_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96556ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_syn_swap(text_list:list,aug_p:float=0.3,aug_min:int=1, aug_max:int=10,n_new_sent=2):\n",
    "    \n",
    "    \"Function to augment text list of lists based on synonyms from wordnet.\"\n",
    "    \n",
    "    #1. Initialize data augmenter\n",
    "    aug = naw.SynonymAug(aug_src='wordnet',aug_p=aug_p,aug_min=aug_min,aug_max=aug_max,stopwords=stopwords)\n",
    "    \n",
    "    #2. Augment\n",
    "    aug_text_list_all=[]\n",
    "    #2.1Loop over text\n",
    "    for t in text_list:\n",
    "        aug_text_list_temp=[]\n",
    "        #2.2 Loop over sentence pairs\n",
    "        for s in t:\n",
    "            # Created augmented synonym sentences based on wordent synonyms\n",
    "            syn_t1=aug.augment(s[0],n=n_new_sent)\n",
    "            syn_t2=aug.augment(s[1],n=n_new_sent)\n",
    "\n",
    "            #2.3 Create new list\n",
    "            for i,j in zip(syn_t1,syn_t2):\n",
    "                aug_text_list_temp.append(tuple((i,j)))\n",
    "                \n",
    "        #3. Append to overall text list        \n",
    "        aug_text_list_all.append(aug_text_list_temp)\n",
    "        \n",
    "    return aug_text_list_all\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0804b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_trans_swap(text_list:list,from_lang='eng',to_lang='de'):\n",
    "    \n",
    "    \"\"\"Text augmentation using reverse translation via google translator.\"\"\"\n",
    "    #1. Loop over texts\n",
    "    aug_text_all=[]\n",
    "    for t in text_list:\n",
    "        \n",
    "        #2. Over sentence pairs within text\n",
    "        aug_text_temp=[]\n",
    "        for s in t:\n",
    "            # Translate from english to germam and then back\n",
    "            s0=ts.google(s[0], from_language='en', to_language='de')\n",
    "            s0_aug=ts.google(s0, from_language='de', to_language='en')\n",
    "            \n",
    "            s1=ts.google(s[1], from_language='en', to_language='de')\n",
    "            s1_aug=ts.google(s1, from_language='de', to_language='en')\n",
    "            \n",
    "            aug_text_temp.append(tuple((s0_aug,s1_aug)))\n",
    "            \n",
    "        aug_text_all.append(aug_text_temp)\n",
    "    \n",
    "    return aug_text_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75ec20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lists(lst):\n",
    "    \n",
    "    \"\"\"\"Function to split list of lists into two lists\"\"\"\n",
    "\n",
    "    l1=[]\n",
    "    l2=[]\n",
    "    for t in lst:\n",
    "        for s in t:\n",
    "            l1.append(s[0])\n",
    "            l2.append(s[1])\n",
    "        \n",
    "    return l1,l2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "fa35cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_BERT_train(pair_text_list:list,random_text_list:list):\n",
    "    \n",
    "    \"\"\"Function to prepare/tokenize data for BERT training/fine tunning\"\"\"\n",
    "    \n",
    "    #1. Split lists\n",
    "    pair_1,pair_2=split_lists(pair_text_list)\n",
    "    pair_labels=list(np.zeros(len(pair_1),np.int32))\n",
    "    \n",
    "    not_pair_1,not_pair_2=split_lists(random_text_list)\n",
    "    notpair_labels=list(np.ones(len(pair_1),np.int32))\n",
    "    \n",
    "    #2. Concatenate examples\n",
    "    sentences_1=pair_1+not_pair_1\n",
    "    sentences_2=pair_2+not_pair_2\n",
    "    labels=pair_labels+notpair_labels\n",
    "    \n",
    "    #3.Tokenize \n",
    "    bert_inputs = tokenizer(sentences_1, sentences_2,return_tensors='pt',truncation=True, padding='max_length')\n",
    "    \n",
    "    #4. Add labels\n",
    "    bert_inputs['next_sentence_label'] = torch.LongTensor([labels]).T #1. NSP fine tunning - next sentence labeling \n",
    "    bert_inputs['labels'] = bert_inputs.input_ids.detach().clone() #2. MLM fine tunning -label cloning\n",
    "    \n",
    "    return bert_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "2aa16f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_BERT_inference(text:list,prep_type:Union['concat','flatten']='flatten'):\n",
    "    \n",
    "    \"\"\"Function to prepare/tokenize data for BERT inference.\"\"\"\n",
    "    \n",
    "    #1. Flatten text list or concatenate text (join)\n",
    "    if prep_type=='flatten':\n",
    "        text_flat=list(np.unique(np.concatenate(text).flat))\n",
    "    else:\n",
    "        text_flat=' '.join(text)\n",
    "    \n",
    "    #2. TOkenize using bert tokenizer\n",
    "    bert_inputs = tokenizer(text_flat,return_tensors='pt',truncation=True, padding='max_length')\n",
    "    \n",
    "    return bert_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "7d10be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_inputids(inputs,mask_prop:float=0.15):\n",
    "    \n",
    "    #1. create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    \n",
    "    #2. create mask array\n",
    "    mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "               (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "    \n",
    "    #3. Get mask only input id indices\n",
    "    selection = []\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    \n",
    "    #4. Mask input ids with 103 token marker    \n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 103\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "a9cc3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set class to load \n",
    "\n",
    "class BertDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    \"\"\"Data loader class required for BERT fine tunner function\"\"\"\n",
    "    \n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "f3b93f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fine tune bert\n",
    "\n",
    "def fine_tune_BERT(model:BertForPreTraining,inputs,epochs:int=5,batch_size:int=16,learning_rate:float=1e-4):\n",
    "    \n",
    "    # Convert bert inputs (dict) returned by BertDataset class to dataset loader object\n",
    "    dataset = BertDataset(inputs)\n",
    "    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model training mode and optmizer\n",
    "    model.train()\n",
    "    optim = AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Loop over epochs acumulate losses\n",
    "    loss_acum=defaultdict(list)\n",
    "    for epoch in range(epochs):\n",
    "        # setup loop with TQDM and dataloader\n",
    "        loop = tqdm(dataset_loader, leave=True)\n",
    "        for i,batch in enumerate(loop):\n",
    "            # initialize calculated gradients (from prev step)\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            # pull all tensor batches required for training\n",
    "            input_ids = batch['input_ids']\n",
    "            token_type_ids = batch['token_type_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            next_sentence_label = batch['next_sentence_label']\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            # process\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            next_sentence_label=next_sentence_label,\n",
    "                            labels=labels)\n",
    "            # extract loss\n",
    "            loss = outputs.loss # NLLloss function (negative log likelihood loss)\n",
    "            # save loss\n",
    "            loss_acum['batch_{}'.format(i)]=loss\n",
    "            # calculate loss for every parameter that needs grad update\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optim.step()\n",
    "            # print relevant info to progress bar\n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "    return model,loss_acum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "e9d8505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide text embedding given bert model and text\n",
    "\n",
    "def pred_emb_BERT(token_ids,segment_ids,attention_mask_ids,model):\n",
    "    \n",
    "    #1. Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "    \n",
    "    #2. Add dummy batch dimension\n",
    "    token_ids=token_ids[None,:]\n",
    "    segment_ids=segment_ids[None,:]\n",
    "    \n",
    "    #2. Produce output\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        outputs=model(token_ids, segment_ids)\n",
    "    \n",
    "    #3. Produce hidden stantes (3rd output)\n",
    "    hidden_states = outputs[2]\n",
    "        \n",
    "    #4. Stack embedding output\n",
    "    token_embeddings = torch.stack(hidden_states[-4:], dim=0) # Stack last 4 years as suggested in research (https://jalammar.github.io/illustrated-bert/)\n",
    "    print('Stacked embedding size {}.'.format(token_embeddings.size()))\n",
    "          \n",
    "    #5. Remove dimension 1 (batches)\n",
    "    #token_embeddings = torch.squeeze(token_embeddings, dim=1).sum(dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    print('Reduced emebedding size {}.'.format(token_embeddings.size()))    \n",
    "    \n",
    "    #6.Attention mask padded tokens\n",
    "    mask=attention_mask_ids.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    masked_embeddings = token_embeddings * mask \n",
    "    masked_embeddings = torch.sum(masked_embeddings, 1) # Extra\n",
    "    print('Mask shape {}.'.format(masked_embeddings.shape))\n",
    "    \n",
    "    summed_mask = torch.clamp(mask.sum(1), min=1e-9) # Extra\n",
    "    mean_pooled = masked_embeddings / summed_mask # Extra\n",
    "    print('Mask shape 2 {}.'.format(mean_pooled.shape)) # Extra\n",
    "    \n",
    "    #7. Average token embeddings to get sentence/paragraph embedding\n",
    "    masked_embeddings_summed = torch.mean(mean_pooled, 0)\n",
    "    print('Final dimension size {}.'.format(masked_embeddings_summed.size()))\n",
    "    \n",
    "    return masked_embeddings_summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "578192c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text \n",
    "\n",
    "def compare_text(emb1,emb2):\n",
    "    \n",
    "    \"\"\"Calculate similarity between embeddings\"\"\"\n",
    "    \n",
    "    cos=torch.nn.CosineSimilarity(dim=0)\n",
    "    \n",
    "    diff_emb = cos(emb1, emb2)\n",
    "    \n",
    "    return diff_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1a525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629bdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "11af5a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2572,  1045,  ...,     0,     0,     0],\n",
       "        [  101,  2064, 11498,  ...,     0,     0,     0],\n",
       "        [  101, 11498,  3401,  ...,     0,     0,     0],\n",
       "        [  101,  2054, 11498,  ...,     0,     0,     0],\n",
       "        [  101,  2339,  3712,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_list_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "18c2f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n"
     ]
    }
   ],
   "source": [
    "d=pred_emb_BERT(questions_list_bert['input_ids'][i],questions_list_bert['token_type_ids'][i],questions_list_bert['attention_mask'][i],m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "85bf81e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5061,  0.9535,  0.1198,  ..., -0.9036,  0.6887, -0.6608],\n",
       "        [ 1.0239,  1.5360,  0.1381,  ..., -1.1146,  0.0461, -1.2879],\n",
       "        [ 0.8100,  1.6151, -0.1458,  ..., -1.5533,  0.2507, -1.4091],\n",
       "        [ 1.1379,  1.1596,  0.1782,  ..., -0.9773,  0.3478, -0.9347]])"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "7be46b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2572,  1045,  ...,     0,     0,     0],\n",
       "        [  101,  2064, 11498,  ...,     0,     0,     0],\n",
       "        [  101, 11498,  3401,  ...,     0,     0,     0],\n",
       "        [  101,  2054, 11498,  ...,     0,     0,     0],\n",
       "        [  101,  2339,  3712,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_list_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee4c1e",
   "metadata": {},
   "source": [
    "### 2. BERT fine tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfd57c",
   "metadata": {},
   "source": [
    "##### 2.1 Initial data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "3b7652e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello, my name is Alice.',\n",
       "  'I’m calling from Chicago and want to ask some questions.',\n",
       "  'I’m pregnant for 6 months now, but I’m not telling anyone about this.',\n",
       "  'I have periodic headaches.',\n",
       "  'When I work, I feel like my ability to concentrate is being hindered by them.',\n",
       "  'It’s already hard to work from 9 to 5 every day, God, and now this.',\n",
       "  'My mom told me about this wonder drug called paracetamol.',\n",
       "  'She assured me that it would help me a lot.',\n",
       "  'I’m not sure if that is okay.',\n",
       "  'It’s not like I’m a specialist in this field or anything so I decided to call here to be sure just in case.',\n",
       "  'Can I use this medicine safely and will it help me?']]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lists (also split into sentences)\n",
    "\n",
    "raw_text_list=txt_to_lists(CORPUS_PATH,to_sentence=True)\n",
    "raw_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "574f5e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Hello, my name is Alice.',\n",
       "   'I’m calling from Chicago and want to ask some questions.'),\n",
       "  ('I’m calling from Chicago and want to ask some questions.',\n",
       "   'I’m pregnant for 6 months now, but I’m not telling anyone about this.'),\n",
       "  ('I’m pregnant for 6 months now, but I’m not telling anyone about this.',\n",
       "   'I have periodic headaches.'),\n",
       "  ('I have periodic headaches.',\n",
       "   'When I work, I feel like my ability to concentrate is being hindered by them.'),\n",
       "  ('When I work, I feel like my ability to concentrate is being hindered by them.',\n",
       "   'It’s already hard to work from 9 to 5 every day, God, and now this.'),\n",
       "  ('It’s already hard to work from 9 to 5 every day, God, and now this.',\n",
       "   'My mom told me about this wonder drug called paracetamol.'),\n",
       "  ('My mom told me about this wonder drug called paracetamol.',\n",
       "   'She assured me that it would help me a lot.'),\n",
       "  ('She assured me that it would help me a lot.',\n",
       "   'I’m not sure if that is okay.'),\n",
       "  ('I’m not sure if that is okay.',\n",
       "   'It’s not like I’m a specialist in this field or anything so I decided to call here to be sure just in case.'),\n",
       "  ('It’s not like I’m a specialist in this field or anything so I decided to call here to be sure just in case.',\n",
       "   'Can I use this medicine safely and will it help me?')]]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of text sentences to list of subsequent sentence pairs\n",
    "\n",
    "raw_sent_pairs=text_list_to_sent_pairs(raw_text_list)\n",
    "raw_sent_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "3f2d33a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('I’m pregnant for 6 months now, but I’m not telling anyone about this.',\n",
       "   'I’m not sure if that is okay.'),\n",
       "  ('I have periodic headaches.',\n",
       "   'She assured me that it would help me a lot.'),\n",
       "  ('Hello, my name is Alice.', 'I have periodic headaches.'),\n",
       "  ('My mom told me about this wonder drug called paracetamol.',\n",
       "   'I’m pregnant for 6 months now, but I’m not telling anyone about this.'),\n",
       "  ('I’m not sure if that is okay.',\n",
       "   'I’m pregnant for 6 months now, but I’m not telling anyone about this.'),\n",
       "  ('She assured me that it would help me a lot.',\n",
       "   'My mom told me about this wonder drug called paracetamol.'),\n",
       "  ('I’m calling from Chicago and want to ask some questions.',\n",
       "   'Can I use this medicine safely and will it help me?'),\n",
       "  ('When I work, I feel like my ability to concentrate is being hindered by them.',\n",
       "   'Hello, my name is Alice.'),\n",
       "  ('It’s already hard to work from 9 to 5 every day, God, and now this.',\n",
       "   'It’s not like I’m a specialist in this field or anything so I decided to call here to be sure just in case.'),\n",
       "  ('It’s not like I’m a specialist in this field or anything so I decided to call here to be sure just in case.',\n",
       "   'I’m calling from Chicago and want to ask some questions.')]]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create balanced data of random sentence pairs (uset all texts all sentences and one random instance pair per sentence)\n",
    "\n",
    "raw_sent_notpairs=sent_pairs_to_random_pairs(raw_sent_pairs,text_resample_size=1.0,sent_resample_size=1.0,n_resamples=1)\n",
    "raw_sent_notpairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce460e52",
   "metadata": {},
   "source": [
    "##### 2.2 No data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "4cdc40cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2043, 1045,  ...,    0,    0,    0],\n",
       "        [ 101, 2009, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 2009, 1521,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]), 'labels': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2043, 1045,  ...,    0,    0,    0],\n",
       "        [ 101, 2009, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 2009, 1521,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat dataset for BERT with no data augmentation\n",
    "\n",
    "bert_inputs_noaug=prepare_data_for_BERT_train(raw_sent_pairs,raw_sent_notpairs)\n",
    "bert_inputs_noaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20ba5925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2009, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 2043, 1045,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 2031,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]), 'labels': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2009, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 2043, 1045,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 2031,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mask input ids for MLP head\n",
    "\n",
    "bert_inputs_noaug_masked=mask_inputids(bert_inputs_noaug,mask_prop=0.15)\n",
    "bert_inputs_noaug_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ac5e7fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]C:\\Users\\gedas\\AppData\\Local\\Temp/ipykernel_47468/3461763534.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:47<00:00, 23.64s/it, loss=14]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:47<00:00, 23.53s/it, loss=9.38]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:49<00:00, 24.86s/it, loss=7.65]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:54<00:00, 27.28s/it, loss=5.72]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:52<00:00, 26.16s/it, loss=4.83]\n"
     ]
    }
   ],
   "source": [
    "# Run bechmark BERT\n",
    "\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
    "m1,loss1=fine_tune_BERT(model,bert_inputs_noaug_masked,epochs=5,batch_size=16,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feabf59d",
   "metadata": {},
   "source": [
    "##### 2.3 Synonym insertation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "726cb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym insertation (pairs) based on wordnet (2 new sentences for each one)\n",
    "\n",
    "raw_sent_pairs_synaug=aug_syn_swap(raw_sent_pairs,aug_p=0.3,aug_min=1, aug_max=10,n_new_sent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "3403e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym insertation (not pairs) based on wordnet (2 new sentences for each one)\n",
    "\n",
    "raw_sent_notpairs_synaug=aug_syn_swap(raw_sent_notpairs,aug_p=0.3,aug_min=1, aug_max=10,n_new_sent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "50eaa098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 6738, 2080,  ...,    0,    0,    0],\n",
       "        [ 101, 7632, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1015, 1521,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2009, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 6738, 2080,  ...,    0,    0,    0],\n",
       "        [ 101, 7632, 1010,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]), 'labels': tensor([[ 101, 6738, 2080,  ...,    0,    0,    0],\n",
       "        [ 101, 7632, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1015, 1521,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2009, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 6738, 2080,  ...,    0,    0,    0],\n",
       "        [ 101, 7632, 1010,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat dataset for BERT with no syn insertaion data augmentation\n",
    "\n",
    "bert_inputs_synaug=prepare_data_for_BERT_train(raw_sent_pairs_synaug,raw_sent_notpairs_synaug)\n",
    "bert_inputs_synaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "c60e82c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]C:\\Users\\gedas\\AppData\\Local\\Temp/ipykernel_25452/3461763534.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 3/3 [01:36<00:00, 32.26s/it, loss=11.9]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 3/3 [01:40<00:00, 33.60s/it, loss=7.08]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 3/3 [01:43<00:00, 34.62s/it, loss=5.09]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 3/3 [01:39<00:00, 33.13s/it, loss=2.91]\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████| 3/3 [01:39<00:00, 33.02s/it, loss=1.6]\n"
     ]
    }
   ],
   "source": [
    "# Run bechmark BERT\n",
    "\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
    "m2,loss2=fine_tune_BERT(model,bert_inputs_synaug,epochs=5,batch_size=16,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb1b35",
   "metadata": {},
   "source": [
    "##### 2.4 Back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "aefdc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym insertation (pairs) based on wordnet (2 new sentences for each one)\n",
    "\n",
    "raw_sent_pairs_transaug=aug_trans_swap(raw_sent_pairs,from_lang='eng',to_lang='de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "6f00b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym insertation (not pairs) based on wordnet (2 new sentences for each one)\n",
    "\n",
    "raw_sent_notpairs_transaug=aug_trans_swap(raw_sent_notpairs,from_lang='eng',to_lang='de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "8da6416a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 2655,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1005,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 1045, 2655,  ...,    0,    0,    0],\n",
       "        [ 101, 2009, 2003,  ...,    0,    0,    0],\n",
       "        [ 101, 7592, 1010,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]), 'labels': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 2655,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1005,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 1045, 2655,  ...,    0,    0,    0],\n",
       "        [ 101, 2009, 2003,  ...,    0,    0,    0],\n",
       "        [ 101, 7592, 1010,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat dataset for BERT with language back translation data augmentation\n",
    "\n",
    "bert_inputs_transaug=prepare_data_for_BERT_train(raw_sent_pairs_transaug,raw_sent_notpairs_transaug)\n",
    "bert_inputs_transaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "34b3822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]C:\\Users\\gedas\\AppData\\Local\\Temp/ipykernel_25452/3461763534.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:45<00:00, 22.86s/it, loss=15.3]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:50<00:00, 25.31s/it, loss=9.82]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:50<00:00, 25.44s/it, loss=6.83]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:57<00:00, 28.62s/it, loss=5.98]\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:52<00:00, 26.27s/it, loss=5.3]\n"
     ]
    }
   ],
   "source": [
    "# Run bechmark BERT\n",
    "\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
    "m3,loss3=fine_tune_BERT(model,bert_inputs_transaug,epochs=5,batch_size=16,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06edf8e2",
   "metadata": {},
   "source": [
    "##### 2.5 Back translation + synonym insertation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "c0bd14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate syn aug and trans aug\n",
    "\n",
    "ran_sent_pairs_combaug=raw_sent_pairs_transaug+raw_sent_pairs_synaug # Combine pair data for synonym augmentation and translation augmentaion\n",
    "raw_sent_notpairs_combaug=raw_sent_notpairs_transaug+raw_sent_notpairs_synaug # Combine not pair data for synonym augmentation and translation augmentaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "6aee17c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 2655,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1005,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2009, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 2592, 2974,  ...,    0,    0,    0],\n",
       "        [ 101, 2009, 1521,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]), 'labels': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 2655,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1005,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2009, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 2592, 2974,  ...,    0,    0,    0],\n",
       "        [ 101, 2009, 1521,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat dataset for BERT with language back translation data augmentation combined with syn insertation augmentaiton\n",
    "\n",
    "bert_inputs_combaug=prepare_data_for_BERT_train(ran_sent_pairs_combaug,raw_sent_notpairs_combaug)\n",
    "bert_inputs_combaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "254adb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]C:\\Users\\gedas\\AppData\\Local\\Temp/ipykernel_47468/3461763534.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:07<00:00, 46.92s/it, loss=10.2]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:57<00:00, 44.37s/it, loss=5.73]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:50<00:00, 42.66s/it, loss=3.33]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:47<00:00, 41.92s/it, loss=1.17]\n",
      "Epoch 4: 100%|███████████████████████████████████████████████████████████████| 4/4 [02:49<00:00, 42.50s/it, loss=0.503]\n",
      "Epoch 5: 100%|███████████████████████████████████████████████████████████████| 4/4 [02:46<00:00, 41.54s/it, loss=0.236]\n",
      "Epoch 6: 100%|███████████████████████████████████████████████████████████████| 4/4 [02:46<00:00, 41.55s/it, loss=0.166]\n",
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:52<00:00, 43.17s/it, loss=0.13]\n",
      "Epoch 8: 100%|██████████████████████████████████████████████████████████████| 4/4 [02:56<00:00, 44.11s/it, loss=0.0958]\n",
      "Epoch 9: 100%|██████████████████████████████████████████████████████████████| 4/4 [03:01<00:00, 45.36s/it, loss=0.0781]\n"
     ]
    }
   ],
   "source": [
    "# Run bechmark BERT\n",
    "\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
    "m4,loss4=fine_tune_BERT(model,bert_inputs_combaug,epochs=10,batch_size=16,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "82262f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'batch_22.818199157714844': tensor(22.8182, grad_fn=<AddBackward0>),\n",
       "             'batch_15.787662506103516': tensor(15.7877, grad_fn=<AddBackward0>),\n",
       "             'batch_12.779033660888672': tensor(12.7790, grad_fn=<AddBackward0>),\n",
       "             'batch_10.189157485961914': tensor(10.1892, grad_fn=<AddBackward0>),\n",
       "             'batch_7.766191482543945': tensor(7.7662, grad_fn=<AddBackward0>),\n",
       "             'batch_7.130096912384033': tensor(7.1301, grad_fn=<AddBackward0>),\n",
       "             'batch_6.189721584320068': tensor(6.1897, grad_fn=<AddBackward0>),\n",
       "             'batch_5.733639717102051': tensor(5.7336, grad_fn=<AddBackward0>),\n",
       "             'batch_5.045542240142822': tensor(5.0455, grad_fn=<AddBackward0>),\n",
       "             'batch_4.438772201538086': tensor(4.4388, grad_fn=<AddBackward0>),\n",
       "             'batch_3.584261417388916': tensor(3.5843, grad_fn=<AddBackward0>),\n",
       "             'batch_3.3339877128601074': tensor(3.3340, grad_fn=<AddBackward0>),\n",
       "             'batch_2.33900785446167': tensor(2.3390, grad_fn=<AddBackward0>),\n",
       "             'batch_1.8993746042251587': tensor(1.8994, grad_fn=<AddBackward0>),\n",
       "             'batch_1.4552485942840576': tensor(1.4552, grad_fn=<AddBackward0>),\n",
       "             'batch_1.1678677797317505': tensor(1.1679, grad_fn=<AddBackward0>),\n",
       "             'batch_0.8840588927268982': tensor(0.8841, grad_fn=<AddBackward0>),\n",
       "             'batch_0.721571683883667': tensor(0.7216, grad_fn=<AddBackward0>),\n",
       "             'batch_0.5829180479049683': tensor(0.5829, grad_fn=<AddBackward0>),\n",
       "             'batch_0.5031662583351135': tensor(0.5032, grad_fn=<AddBackward0>),\n",
       "             'batch_0.37136316299438477': tensor(0.3714, grad_fn=<AddBackward0>),\n",
       "             'batch_0.3366585373878479': tensor(0.3367, grad_fn=<AddBackward0>),\n",
       "             'batch_0.46063631772994995': tensor(0.4606, grad_fn=<AddBackward0>),\n",
       "             'batch_0.23572425544261932': tensor(0.2357, grad_fn=<AddBackward0>),\n",
       "             'batch_0.2345232367515564': tensor(0.2345, grad_fn=<AddBackward0>),\n",
       "             'batch_0.2079375833272934': tensor(0.2079, grad_fn=<AddBackward0>),\n",
       "             'batch_0.3945048153400421': tensor(0.3945, grad_fn=<AddBackward0>),\n",
       "             'batch_0.16619844734668732': tensor(0.1662, grad_fn=<AddBackward0>),\n",
       "             'batch_0.1609819233417511': tensor(0.1610, grad_fn=<AddBackward0>),\n",
       "             'batch_0.14953549206256866': tensor(0.1495, grad_fn=<AddBackward0>),\n",
       "             'batch_0.14525233209133148': tensor(0.1453, grad_fn=<AddBackward0>),\n",
       "             'batch_0.12959876656532288': tensor(0.1296, grad_fn=<AddBackward0>),\n",
       "             'batch_0.13764189183712006': tensor(0.1376, grad_fn=<AddBackward0>),\n",
       "             'batch_0.10553167760372162': tensor(0.1055, grad_fn=<AddBackward0>),\n",
       "             'batch_0.11848115175962448': tensor(0.1185, grad_fn=<AddBackward0>),\n",
       "             'batch_0.0957653746008873': tensor(0.0958, grad_fn=<AddBackward0>),\n",
       "             'batch_0.09363972395658493': tensor(0.0936, grad_fn=<AddBackward0>),\n",
       "             'batch_0.09404975175857544': tensor(0.0940, grad_fn=<AddBackward0>),\n",
       "             'batch_0.09415535628795624': tensor(0.0942, grad_fn=<AddBackward0>),\n",
       "             'batch_0.07811230421066284': tensor(0.0781, grad_fn=<AddBackward0>)})"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee82754",
   "metadata": {},
   "source": [
    "### 3. BERT testing using Questions db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d3514",
   "metadata": {},
   "source": [
    "##### 3.1 Prepare questions embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "6497f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions list\n",
    "\n",
    "questions_list=txt_to_lists(QNA_PATH,to_sentence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "83c597c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare qna text for inference\n",
    "\n",
    "questions_list_bert=prepare_data_for_BERT_inference(questions_list,prep_type='flatten')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07f70e",
   "metadata": {},
   "source": [
    "##### 3.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "650a7d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n"
     ]
    }
   ],
   "source": [
    "# Get question emebeddings\n",
    "\n",
    "question_embedding=torch.empty(questions_list_bert['input_ids'].shape[0],768)\n",
    "for i in range(questions_list_bert['input_ids'].shape[0]):\n",
    "    questions_list_bert_embeddings=pred_emb_BERT(questions_list_bert['input_ids'][i],questions_list_bert['token_type_ids'][i],questions_list_bert['attention_mask'][i],m4)\n",
    "    question_embedding[i]=questions_list_bert_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "b12d52bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_list_bert_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "d9cbfd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n",
      "Stacked embedding size torch.Size([4, 1, 512, 768]).\n",
      "Reduced emebedding size torch.Size([4, 512, 768]).\n",
      "Mask shape torch.Size([4, 768]).\n",
      "Mask shape 2 torch.Size([4, 768]).\n",
      "Final dimension size torch.Size([768]).\n"
     ]
    }
   ],
   "source": [
    "# Loop over text\n",
    "\n",
    "res_dict={}\n",
    "# Loop over texts\n",
    "for i in range(len(raw_text_list)):\n",
    "    \n",
    "    res_dict['query_{}'.format(i)]={}\n",
    "    # Loop over lines and evaluate one after another\n",
    "    for j in range(len(raw_text_list[i])):\n",
    "        # Tokenize\n",
    "        query_list=raw_text_list[i][:j+1]\n",
    "        query_list_bert=prepare_data_for_BERT_inference(query_list,prep_type='concat')\n",
    "        query_embedding=pred_emb_BERT(query_list_bert['input_ids'][0],query_list_bert['token_type_ids'][0],query_list_bert['attention_mask'][0],m4)\n",
    "        \n",
    "        # Calc cos similarity\n",
    "        sim_list=[('q_{}'.format(i),compare_text(query_embedding,emb)) for i,emb in enumerate(question_embedding)]\n",
    "        sim_list.sort(key = lambda x: x[1],reverse=True)\n",
    "\n",
    "        # Update results dict\n",
    "        res_dict['query_{}'.format(i)]['{}_sentences'.format(j)]=sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "b8fdf94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_0': {'0_sentences': [('q_5', tensor(0.9993)),\n",
       "   ('q_4', tensor(0.9992)),\n",
       "   ('q_2', tensor(0.9992)),\n",
       "   ('q_3', tensor(0.9990)),\n",
       "   ('q_1', tensor(0.9988)),\n",
       "   ('q_0', tensor(0.9986))],\n",
       "  '1_sentences': [('q_4', tensor(0.9989)),\n",
       "   ('q_2', tensor(0.9989)),\n",
       "   ('q_3', tensor(0.9987)),\n",
       "   ('q_1', tensor(0.9987)),\n",
       "   ('q_5', tensor(0.9986)),\n",
       "   ('q_0', tensor(0.9985))],\n",
       "  '2_sentences': [('q_4', tensor(0.9967)),\n",
       "   ('q_2', tensor(0.9965)),\n",
       "   ('q_3', tensor(0.9963)),\n",
       "   ('q_1', tensor(0.9963)),\n",
       "   ('q_5', tensor(0.9962)),\n",
       "   ('q_0', tensor(0.9961))],\n",
       "  '3_sentences': [('q_4', tensor(0.9960)),\n",
       "   ('q_2', tensor(0.9958)),\n",
       "   ('q_1', tensor(0.9956)),\n",
       "   ('q_3', tensor(0.9956)),\n",
       "   ('q_5', tensor(0.9954)),\n",
       "   ('q_0', tensor(0.9953))],\n",
       "  '4_sentences': [('q_4', tensor(0.9921)),\n",
       "   ('q_2', tensor(0.9920)),\n",
       "   ('q_3', tensor(0.9915)),\n",
       "   ('q_1', tensor(0.9915)),\n",
       "   ('q_5', tensor(0.9913)),\n",
       "   ('q_0', tensor(0.9911))],\n",
       "  '5_sentences': [('q_2', tensor(0.9858)),\n",
       "   ('q_4', tensor(0.9857)),\n",
       "   ('q_3', tensor(0.9849)),\n",
       "   ('q_1', tensor(0.9848)),\n",
       "   ('q_5', tensor(0.9848)),\n",
       "   ('q_0', tensor(0.9844))],\n",
       "  '6_sentences': [('q_4', tensor(0.9820)),\n",
       "   ('q_2', tensor(0.9819)),\n",
       "   ('q_3', tensor(0.9813)),\n",
       "   ('q_1', tensor(0.9812)),\n",
       "   ('q_5', tensor(0.9812)),\n",
       "   ('q_0', tensor(0.9807))],\n",
       "  '7_sentences': [('q_4', tensor(0.9768)),\n",
       "   ('q_2', tensor(0.9766)),\n",
       "   ('q_3', tensor(0.9760)),\n",
       "   ('q_5', tensor(0.9760)),\n",
       "   ('q_1', tensor(0.9759)),\n",
       "   ('q_0', tensor(0.9753))],\n",
       "  '8_sentences': [('q_4', tensor(0.9726)),\n",
       "   ('q_2', tensor(0.9724)),\n",
       "   ('q_5', tensor(0.9719)),\n",
       "   ('q_3', tensor(0.9718)),\n",
       "   ('q_1', tensor(0.9715)),\n",
       "   ('q_0', tensor(0.9709))],\n",
       "  '9_sentences': [('q_2', tensor(0.9603)),\n",
       "   ('q_4', tensor(0.9602)),\n",
       "   ('q_5', tensor(0.9592)),\n",
       "   ('q_3', tensor(0.9591)),\n",
       "   ('q_1', tensor(0.9588)),\n",
       "   ('q_0', tensor(0.9581))],\n",
       "  '10_sentences': [('q_2', tensor(0.9581)),\n",
       "   ('q_4', tensor(0.9581)),\n",
       "   ('q_5', tensor(0.9571)),\n",
       "   ('q_3', tensor(0.9569)),\n",
       "   ('q_1', tensor(0.9566)),\n",
       "   ('q_0', tensor(0.9559))]}}"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655ea36",
   "metadata": {},
   "source": [
    "##### 3.3 Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "d55a0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "torch.save(m4, MODEL_PATH.joinpath('best_model'))\n",
    "torch.save(tokenizer, MODEL_PATH.joinpath('best_model_t'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "ef8c3517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "#model = torch.load(MODEL_PATH.joinpath('best_model'))\n",
    "#model_t = torch.load(MODEL_PATH.joinpath('best_model_t'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
