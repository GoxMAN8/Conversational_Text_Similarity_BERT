{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771b49bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using state Vilnius server backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "\n",
    "from BERTpredictor import *\n",
    "from BERTtuner import *\n",
    "from DataPreper import *\n",
    "from QueryMatcher import *\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a78bb",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "##### Objective\n",
    "\n",
    "The goal of this project is to be able to predict relevant questions from natural language query/text.\n",
    "\n",
    "##### Methods\n",
    "\n",
    "There are various aproaches for semantic language similarity task. Common apporach is using embedding vectors of words, and them comparing high dimensional similarity between embedding vectors. Popular methods such as Word2Vec and Glove does a great job in representing words as vectors, however they are limited by the fact that words get a constant vector, irrespective of a context that words apear in. More modern approaches, specifically transformer-like methods such as Bert, GPT-2 or XLnet allows for flexible vectors depending on the context it apears on. Such models often come semi-ready for a lot of tasks, being pretrained on large corpuses. Nonetheless they are flexible to be fine-tuned for the task in hand by retraining/fine-tuning parameters using domain specific corpus. Approach taken in this project is to use BERT (base case uncased) fine tuned on domain corpus, which are conversational logs of clients seeking medical advice. \n",
    "\n",
    "Base-case uncased BERT model is trained on large wikipedia corpus and contains 110M parameters. There are other potentially promising pre-trained transformer models such as DeepPavlov/bert-base-cased-conversational, which are trained on conversational natural language texts, however testing those is beyond the scope of this project. Also there are significant number of ways text data could be prepared/engineered and vector similarity metrics. Some of them were explored here, however choices are abundant and user is free to expand, experiment further. It is important to note that embedding calculation uses sum from last 4 (out of 12) layers as suggested in research giving most accurate representation.\n",
    "\n",
    "![](1_fWh1m6FyC6bAs3Qfh9iVmg.png)\n",
    "\n",
    "##### Results\n",
    "\n",
    "BERT model had been fine-tuned using combined NSP/MLP methodology (https://huggingface.co/bert-base-uncased), on a tiny dummy corpus. Results and usefulness of the model seem ambigous, however framework, if used with more extensive dataset should provide more promsing results.  \n",
    "\n",
    "### BERT fine tunning (section 2)\n",
    "\n",
    "\n",
    "This is the main section for BERT model fine-tunning. Few different data augmentation strategies experimented with basic/default BERT hyperparemeters to establish benchmark model. \n",
    "\n",
    "##### Initial data prep\n",
    "\n",
    "One of the necessary steps in data preparation for tunning is bert is to create subsequent/random sentence pairs (NSP head) in the from (sentence, subsequent sentence). Balanced data had been created with one random pair for every original pair. There are ways to improve by creating random pairs chosing sentences from different logs, however in the absense of training data random pairs had been created from the same log.\n",
    "\n",
    "##### No data augmentation\n",
    "\n",
    "In order to do intial tests no data augmentation was implemented. Default parameters was used. Negative combined (NLP + MLP) log likelihood loss was calculated on training data for each epoch which droped to 4.85 after 5 epochs.\n",
    "\n",
    "##### Synonym insertion\n",
    "\n",
    "In order to create first augmented dataset a synonym insertation was used based on NLTK (wordnet) package. (https://www.holisticseo.digital/python-seo/nltk/wordnet)  Stopwords had been removed, before changing 0.3 words with synonyms no less than 1 word and no more than 10. One augmented sentence created for every original sentence. Training loss dropped to 4.7 after 5 epochs with original parameters.\n",
    "\n",
    "##### Back translation\n",
    "\n",
    "Second augmented dataset was created by translating sentences to foreign language (german was used in this study), and then back. Module used - https://pypi.org/project/translators/. One augmented sentence created for every original sentence. Training loss dropped to 4.85 after 5 epochs with original parameters.\n",
    "\n",
    "##### Back translation + synonym insertation\n",
    "\n",
    "Two augmentation techniques combined to create an extended dataset (3x the original). Training loss dropped to 4.7 after 5 epochs with original parameters.\n",
    "\n",
    "### BERT optimization via Bayes opt (section 3)\n",
    "\n",
    "In order to find optimal hyperparameters Bayesian Optimization was chosen due to very expensive evaluation. This is the method to squeeze the last juice out of ML model, however should come after extensive experimentation with feature engineering and data gathering. Bayesian optimization based on - https://github.com/fmfn/BayesianOptimization. \n",
    "\n",
    "##### Prepare loss function\n",
    "\n",
    "In order to evaluate model a common function from information retrieval theory had been chosen which is **top k precision**. Meaning the proportion of relevant documents (questions) in top k retrieved/predicted documents (questions). \n",
    "\n",
    "##### Prepare objective function\n",
    "\n",
    "Objective function to be maximized (returning **top k precision**) had been created which takes optimizeable parameters **epochs, batch size, learning rate**. It is possible to expand objective function by adding customizeable data preparation techniques, however for the purpose of this project objective function maintained smaller, and best feature engineering type was chosen in previous section based on results on training data.\n",
    "\n",
    "##### Optimize\n",
    "\n",
    "Bayesian optmization engine initiaed with 3 random initializations of parameters (within provided range) and 15 expectation maximization iterations. **Best parameters : batch_size=14 (set 16 for final model), epochs=9, learning_rate=0.0063**.\n",
    "\n",
    "### Train and save best model (section 4)\n",
    "\n",
    "Use parameters inferred from previous section to train the best model. Best model along with tokenizer saved to local folder, together with question embeddings for quick access for a command line app. \n",
    "\n",
    "### Conclusions and discussion\n",
    "\n",
    "There were few challenges faced. First of all, different aggregations of BERT hidden states had been experimented with involving different number of layers and different functions, however similar tendency observed of overly high and non-differentiating similarity scores between queries and questions, decreasing with increasing query length. (as more sentences withing query observed) Result could be severe undertraining with words triggering similar neurons. Also, there could be some unknown bug in embedding aggregation which need to be investigated. One more reason could be wrong method chosen, it is likely that framing problem to have a classification head (multilabelled data for training aka tag problem) would improve embedding accuracy. It is likely that for the method to be used for production with low latency, bi-directional transformer should be trained from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c383c5d",
   "metadata": {},
   "source": [
    "### 1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793f91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "CORPUS_PATH=pathlib.Path().absolute().joinpath('corpus') # Path to text corpus (conversational logs in .txt files)\n",
    "CORPUS_AUG_PATH=pathlib.Path().absolute().joinpath('corpus_aug') # Augmented text corpus - synthetically edited corpus\n",
    "CORPUS_LABELLED=pathlib.Path().absolute().joinpath('corpus_labelled') # Labelled corpus\n",
    "\n",
    "QNA_PATH=pathlib.Path().absolute().joinpath('questions') # Path to questions\n",
    "\n",
    "MODEL_PATH=pathlib.Path().absolute().joinpath('models') # Path to trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee4c1e",
   "metadata": {},
   "source": [
    "### 2. BERT fine tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfd57c",
   "metadata": {},
   "source": [
    "##### 2.1 Initial data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5746b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data_prepper class\n",
    "\n",
    "data_prepper=DataPrepper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7652e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gedas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Hello, my name is Alice.',\n",
       "  'I’m calling from Chicago and want to ask some questions.',\n",
       "  'I’m pregnant for 6 months now, but I’m not telling anyone about this.',\n",
       "  'I have periodic headaches.',\n",
       "  'When I work, I feel like my ability to concentrate is being hindered by them.',\n",
       "  'It’s already hard to work from 9 to 5 every day, God, and now this.',\n",
       "  'My mom told me about this wonder drug called paracetamol.',\n",
       "  'She assured me that it would help me a lot.',\n",
       "  'I’m not sure if that is okay.',\n",
       "  'It’s not like I’m a specialist in this field or anything so I decided to call here to be sure just in case.',\n",
       "  'Can I use this medicine safely and will it help me?']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corpus text to lists (also split into sentences)\n",
    "\n",
    "raw_text_list=data_prepper.txt_to_lists(CORPUS_PATH,to_sentence=True)\n",
    "raw_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "574f5e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Hello, my name is Alice.',\n",
       "   'I’m calling from Chicago and want to ask some questions.'),\n",
       "  ('I’m calling from Chicago and want to ask some questions.',\n",
       "   'I’m pregnant for 6 months now, but I’m not telling anyone about this.'),\n",
       "  ('I’m pregnant for 6 months now, but I’m not telling anyone about this.',\n",
       "   'I have periodic headaches.'),\n",
       "  ('I have periodic headaches.',\n",
       "   'When I work, I feel like my ability to concentrate is being hindered by them.'),\n",
       "  ('When I work, I feel like my ability to concentrate is being hindered by them.',\n",
       "   'It’s already hard to work from 9 to 5 every day, God, and now this.'),\n",
       "  ('It’s already hard to work from 9 to 5 every day, God, and now this.',\n",
       "   'My mom told me about this wonder drug called paracetamol.'),\n",
       "  ('My mom told me about this wonder drug called paracetamol.',\n",
       "   'She assured me that it would help me a lot.'),\n",
       "  ('She assured me that it would help me a lot.',\n",
       "   'I’m not sure if that is okay.'),\n",
       "  ('I’m not sure if that is okay.',\n",
       "   'It’s not like I’m a specialist in this field or anything so I decided to call here to be sure just in case.'),\n",
       "  ('It’s not like I’m a specialist in this field or anything so I decided to call here to be sure just in case.',\n",
       "   'Can I use this medicine safely and will it help me?')]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of text sentences to list of subsequent sentence pairs\n",
    "\n",
    "raw_sent_pairs=data_prepper.text_list_to_sent_pairs(raw_text_list)\n",
    "raw_sent_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2d33a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('She assured me that it would help me a lot.',\n",
       "   'My mom told me about this wonder drug called paracetamol.'),\n",
       "  ('Hello, my name is Alice.', 'I’m not sure if that is okay.'),\n",
       "  ('It’s not like I’m a specialist in this field or anything so I decided to call here to be sure just in case.',\n",
       "   'My mom told me about this wonder drug called paracetamol.'),\n",
       "  ('I have periodic headaches.',\n",
       "   'Can I use this medicine safely and will it help me?'),\n",
       "  ('I’m calling from Chicago and want to ask some questions.',\n",
       "   'I’m not sure if that is okay.'),\n",
       "  ('It’s already hard to work from 9 to 5 every day, God, and now this.',\n",
       "   'Can I use this medicine safely and will it help me?'),\n",
       "  ('I’m pregnant for 6 months now, but I’m not telling anyone about this.',\n",
       "   'Hello, my name is Alice.'),\n",
       "  ('When I work, I feel like my ability to concentrate is being hindered by them.',\n",
       "   'Can I use this medicine safely and will it help me?'),\n",
       "  ('I’m not sure if that is okay.',\n",
       "   'Can I use this medicine safely and will it help me?'),\n",
       "  ('My mom told me about this wonder drug called paracetamol.',\n",
       "   'It’s not like I’m a specialist in this field or anything so I decided to call here to be sure just in case.')]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create balanced data of random sentence pairs (uset all texts all sentences and one random instance pair per sentence)\n",
    "\n",
    "raw_sent_notpairs=data_prepper.sent_pairs_to_random_pairs(raw_sent_pairs,text_resample_size=1.0,sent_resample_size=1.0,n_resamples=1)\n",
    "raw_sent_notpairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f196ce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gedas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Can paracetamol be used during pregnancy?',\n",
       "  'What paracetamol is used for?',\n",
       "  'What are paracetamol interactions with other medicine?',\n",
       "  'Why is it raining today?',\n",
       "  'When the war in Ukraine will end?',\n",
       "  'What is the oldest town in London?']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse question files\n",
    "\n",
    "questions_list=data_prepper.txt_to_lists(QNA_PATH)\n",
    "questions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce460e52",
   "metadata": {},
   "source": [
    "##### 2.2 No data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a387cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:871: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xe (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  obj = cast(Storage, torch._UntypedStorage(nbytes))\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT tuner class\n",
    "\n",
    "BERT_tuner=BERTtuner()\n",
    "BERT_tuner.load_from_web(model_name='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cdc40cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 1045,  103,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2043,  103,  ...,    0,    0,    0],\n",
       "        [ 101,  103, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 2026, 3566,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]), 'labels': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2043, 1045,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 2026, 3566,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat dataset for BERT with no data augmentation (add pairs and not pairs)\n",
    "\n",
    "bert_inputs_noaug=BERT_tuner.prepare_data_for_BERT_train(raw_sent_pairs,raw_sent_notpairs)\n",
    "bert_inputs_noaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac5e7fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:48<00:00, 24.45s/it, loss=14]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:55<00:00, 27.57s/it, loss=10.1]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:55<00:00, 27.82s/it, loss=7.07]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:56<00:00, 28.44s/it, loss=6.06]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:55<00:00, 27.53s/it, loss=4.85]\n"
     ]
    }
   ],
   "source": [
    "# Run bechmark BERT\n",
    "\n",
    "m1,loss1=BERT_tuner.train_BERT(bert_inputs_noaug,epochs=5,batch_size=16,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "741f0b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2064, 11498,  ...,     0,     0,     0],\n",
       "        [  101,  2054,  2024,  ...,     0,     0,     0],\n",
       "        [  101,  2054,  2003,  ...,     0,     0,     0],\n",
       "        [  101,  2054, 11498,  ...,     0,     0,     0],\n",
       "        [  101,  2043,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  2339,  2003,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'embeddings': tensor([[ 0.0815,  0.1221,  0.0295,  ..., -0.1476, -0.0154, -0.0385],\n",
       "        [ 0.0842,  0.1325,  0.0305,  ..., -0.1609, -0.0156, -0.0422],\n",
       "        [ 0.0618,  0.1013,  0.0190,  ..., -0.1246, -0.0148, -0.0355],\n",
       "        [ 0.0713,  0.1115,  0.0257,  ..., -0.1361, -0.0137, -0.0335],\n",
       "        [ 0.0672,  0.1022,  0.0220,  ..., -0.1252, -0.0156, -0.0367],\n",
       "        [ 0.0509,  0.0824,  0.0159,  ..., -0.1004, -0.0093, -0.0258]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare question embeddings using newly trained model (query question)\n",
    "\n",
    "query_matcher=QueryMatcher(m1,BERT_tuner.model_tokenizer,False,True) # Init query matcher class\n",
    "#query_matcher.parse_questions_files(save_index=True) # Save question index (first run only)\n",
    "#query_matcher.parse_queries_files(save_index=True) # Save queries index (first run only)\n",
    "question_embeddings=query_matcher.calc_question_embeddings(questions_list,save=False) # Calcuate embeddings\n",
    "question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e71e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_1.txt': {'0_sentences': [('questions_1.txt_2', tensor(0.9992)),\n",
       "   ('questions_1.txt_5', tensor(0.9991)),\n",
       "   ('questions_1.txt_4', tensor(0.9990)),\n",
       "   ('questions_1.txt_3', tensor(0.9987)),\n",
       "   ('questions_1.txt_1', tensor(0.9982)),\n",
       "   ('questions_1.txt_0', tensor(0.9978))],\n",
       "  '1_sentences': [('questions_1.txt_4', tensor(0.9982)),\n",
       "   ('questions_1.txt_2', tensor(0.9981)),\n",
       "   ('questions_1.txt_3', tensor(0.9979)),\n",
       "   ('questions_1.txt_5', tensor(0.9978)),\n",
       "   ('questions_1.txt_1', tensor(0.9977)),\n",
       "   ('questions_1.txt_0', tensor(0.9973))],\n",
       "  '2_sentences': [('questions_1.txt_4', tensor(0.9938)),\n",
       "   ('questions_1.txt_2', tensor(0.9934)),\n",
       "   ('questions_1.txt_5', tensor(0.9932)),\n",
       "   ('questions_1.txt_3', tensor(0.9931)),\n",
       "   ('questions_1.txt_1', tensor(0.9929)),\n",
       "   ('questions_1.txt_0', tensor(0.9923))],\n",
       "  '3_sentences': [('questions_1.txt_4', tensor(0.9923)),\n",
       "   ('questions_1.txt_2', tensor(0.9918)),\n",
       "   ('questions_1.txt_5', tensor(0.9916)),\n",
       "   ('questions_1.txt_3', tensor(0.9915)),\n",
       "   ('questions_1.txt_1', tensor(0.9913)),\n",
       "   ('questions_1.txt_0', tensor(0.9907))],\n",
       "  '4_sentences': [('questions_1.txt_4', tensor(0.9868)),\n",
       "   ('questions_1.txt_2', tensor(0.9864)),\n",
       "   ('questions_1.txt_5', tensor(0.9862)),\n",
       "   ('questions_1.txt_3', tensor(0.9858)),\n",
       "   ('questions_1.txt_1', tensor(0.9854)),\n",
       "   ('questions_1.txt_0', tensor(0.9847))],\n",
       "  '5_sentences': [('questions_1.txt_4', tensor(0.9776)),\n",
       "   ('questions_1.txt_2', tensor(0.9771)),\n",
       "   ('questions_1.txt_5', tensor(0.9767)),\n",
       "   ('questions_1.txt_3', tensor(0.9761)),\n",
       "   ('questions_1.txt_1', tensor(0.9753)),\n",
       "   ('questions_1.txt_0', tensor(0.9744))],\n",
       "  '6_sentences': [('questions_1.txt_4', tensor(0.9686)),\n",
       "   ('questions_1.txt_2', tensor(0.9680)),\n",
       "   ('questions_1.txt_5', tensor(0.9677)),\n",
       "   ('questions_1.txt_3', tensor(0.9671)),\n",
       "   ('questions_1.txt_1', tensor(0.9662)),\n",
       "   ('questions_1.txt_0', tensor(0.9652))],\n",
       "  '7_sentences': [('questions_1.txt_4', tensor(0.9594)),\n",
       "   ('questions_1.txt_2', tensor(0.9588)),\n",
       "   ('questions_1.txt_5', tensor(0.9584)),\n",
       "   ('questions_1.txt_3', tensor(0.9576)),\n",
       "   ('questions_1.txt_1', tensor(0.9565)),\n",
       "   ('questions_1.txt_0', tensor(0.9554))],\n",
       "  '8_sentences': [('questions_1.txt_4', tensor(0.9507)),\n",
       "   ('questions_1.txt_2', tensor(0.9501)),\n",
       "   ('questions_1.txt_5', tensor(0.9496)),\n",
       "   ('questions_1.txt_3', tensor(0.9487)),\n",
       "   ('questions_1.txt_1', tensor(0.9473)),\n",
       "   ('questions_1.txt_0', tensor(0.9461))],\n",
       "  '9_sentences': [('questions_1.txt_4', tensor(0.9280)),\n",
       "   ('questions_1.txt_2', tensor(0.9278)),\n",
       "   ('questions_1.txt_5', tensor(0.9266)),\n",
       "   ('questions_1.txt_3', tensor(0.9256)),\n",
       "   ('questions_1.txt_1', tensor(0.9237)),\n",
       "   ('questions_1.txt_0', tensor(0.9222))],\n",
       "  '10_sentences': [('questions_1.txt_4', tensor(0.9188)),\n",
       "   ('questions_1.txt_2', tensor(0.9188)),\n",
       "   ('questions_1.txt_5', tensor(0.9173)),\n",
       "   ('questions_1.txt_3', tensor(0.9163)),\n",
       "   ('questions_1.txt_1', tensor(0.9143)),\n",
       "   ('questions_1.txt_0', tensor(0.9127))]}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run query matcher\n",
    "\n",
    "res_1=query_matcher.match_queries(raw_text_list,prep_type='concat')\n",
    "res_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feabf59d",
   "metadata": {},
   "source": [
    "##### 2.3 Synonym insertation (augmentation technique 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "726cb7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gedas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Synonym insertation (pairs) based on wordnet (2 new sentences for each one)\n",
    "\n",
    "raw_sent_pairs_synaug=data_prepper.aug_syn_swap(text_list=raw_sent_pairs,aug_p=0.3,aug_min=1, aug_max=10,n_new_sent=2,path=CORPUS_AUG_PATH,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3403e026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gedas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Synonym insertation (not pairs) based on wordnet (2 new sentences for each one)\n",
    "\n",
    "raw_sent_notpairs_synaug=data_prepper.aug_syn_swap(raw_sent_notpairs,aug_p=0.3,aug_min=1, aug_max=10,n_new_sent=2,path=CORPUS_AUG_PATH,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50eaa098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592,  103,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2043, 1045,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 2016,  103,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]), 'labels': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2043, 1045,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1521,  ...,    0,    0,    0],\n",
       "        [ 101, 2016, 8916,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat dataset for BERT with no syn insertaion data augmentation\n",
    "\n",
    "BERT_tuner=BERTtuner()\n",
    "BERT_tuner.load_from_web(model_name='bert-base-uncased')\n",
    "\n",
    "bert_inputs_synaug=BERT_tuner.prepare_data_for_BERT_train(raw_sent_pairs,raw_sent_notpairs)\n",
    "bert_inputs_synaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c60e82c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:52<00:00, 26.43s/it, loss=14.6]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:54<00:00, 27.42s/it, loss=9.17]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:52<00:00, 26.13s/it, loss=7.53]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:56<00:00, 28.48s/it, loss=5.93]\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:51<00:00, 25.56s/it, loss=4.7]\n"
     ]
    }
   ],
   "source": [
    "# Run BERT with augmented data via synonym insertation\n",
    "\n",
    "m2,loss2=BERT_tuner.train_BERT(bert_inputs_synaug,epochs=5,batch_size=16,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c5c30b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gedas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2064, 11498,  ...,     0,     0,     0],\n",
       "        [  101,  2054,  2024,  ...,     0,     0,     0],\n",
       "        [  101,  2054,  2003,  ...,     0,     0,     0],\n",
       "        [  101,  2054, 11498,  ...,     0,     0,     0],\n",
       "        [  101,  2043,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  2339,  2003,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'embeddings': tensor([[ 0.1025,  0.1519,  0.0126,  ..., -0.1515,  0.0138, -0.0596],\n",
       "        [ 0.1049,  0.1653,  0.0110,  ..., -0.1642,  0.0158, -0.0644],\n",
       "        [ 0.0765,  0.1268,  0.0038,  ..., -0.1259,  0.0094, -0.0534],\n",
       "        [ 0.0890,  0.1392,  0.0097,  ..., -0.1392,  0.0135, -0.0519],\n",
       "        [ 0.0823,  0.1277,  0.0076,  ..., -0.1267,  0.0085, -0.0551],\n",
       "        [ 0.0628,  0.1022,  0.0044,  ..., -0.1007,  0.0102, -0.0402]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare question embeddings using newly trained model (query question)\n",
    "\n",
    "query_matcher=QueryMatcher(m2,BERT_tuner.model_tokenizer,False,True) # Init query matcher class\n",
    "question_embeddings=query_matcher.calc_question_embeddings(questions_list,save=False) # Calcuate embeddings\n",
    "question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1bd698a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_0': {'0_sentences': [('q_2', tensor(0.9988)),\n",
       "   ('q_5', tensor(0.9987)),\n",
       "   ('q_4', tensor(0.9987)),\n",
       "   ('q_3', tensor(0.9983)),\n",
       "   ('q_1', tensor(0.9977)),\n",
       "   ('q_0', tensor(0.9973))],\n",
       "  '1_sentences': [('q_4', tensor(0.9979)),\n",
       "   ('q_3', tensor(0.9976)),\n",
       "   ('q_2', tensor(0.9976)),\n",
       "   ('q_5', tensor(0.9973)),\n",
       "   ('q_1', tensor(0.9973)),\n",
       "   ('q_0', tensor(0.9969))],\n",
       "  '2_sentences': [('q_4', tensor(0.9933)),\n",
       "   ('q_3', tensor(0.9926)),\n",
       "   ('q_2', tensor(0.9925)),\n",
       "   ('q_5', tensor(0.9923)),\n",
       "   ('q_1', tensor(0.9921)),\n",
       "   ('q_0', tensor(0.9918))],\n",
       "  '3_sentences': [('q_4', tensor(0.9919)),\n",
       "   ('q_3', tensor(0.9912)),\n",
       "   ('q_2', tensor(0.9910)),\n",
       "   ('q_5', tensor(0.9908)),\n",
       "   ('q_1', tensor(0.9906)),\n",
       "   ('q_0', tensor(0.9903))],\n",
       "  '4_sentences': [('q_4', tensor(0.9866)),\n",
       "   ('q_2', tensor(0.9857)),\n",
       "   ('q_3', tensor(0.9856)),\n",
       "   ('q_5', tensor(0.9854)),\n",
       "   ('q_1', tensor(0.9848)),\n",
       "   ('q_0', tensor(0.9844))],\n",
       "  '5_sentences': [('q_4', tensor(0.9762)),\n",
       "   ('q_2', tensor(0.9753)),\n",
       "   ('q_5', tensor(0.9748)),\n",
       "   ('q_3', tensor(0.9747)),\n",
       "   ('q_1', tensor(0.9734)),\n",
       "   ('q_0', tensor(0.9729))],\n",
       "  '6_sentences': [('q_4', tensor(0.9659)),\n",
       "   ('q_2', tensor(0.9647)),\n",
       "   ('q_3', tensor(0.9643)),\n",
       "   ('q_5', tensor(0.9642)),\n",
       "   ('q_1', tensor(0.9628)),\n",
       "   ('q_0', tensor(0.9622))],\n",
       "  '7_sentences': [('q_4', tensor(0.9553)),\n",
       "   ('q_2', tensor(0.9540)),\n",
       "   ('q_5', tensor(0.9534)),\n",
       "   ('q_3', tensor(0.9532)),\n",
       "   ('q_1', tensor(0.9515)),\n",
       "   ('q_0', tensor(0.9509))],\n",
       "  '8_sentences': [('q_4', tensor(0.9456)),\n",
       "   ('q_2', tensor(0.9444)),\n",
       "   ('q_5', tensor(0.9435)),\n",
       "   ('q_3', tensor(0.9432)),\n",
       "   ('q_1', tensor(0.9412)),\n",
       "   ('q_0', tensor(0.9405))],\n",
       "  '9_sentences': [('q_4', tensor(0.9203)),\n",
       "   ('q_2', tensor(0.9194)),\n",
       "   ('q_5', tensor(0.9175)),\n",
       "   ('q_3', tensor(0.9171)),\n",
       "   ('q_1', tensor(0.9146)),\n",
       "   ('q_0', tensor(0.9137))],\n",
       "  '10_sentences': [('q_4', tensor(0.9086)),\n",
       "   ('q_2', tensor(0.9079)),\n",
       "   ('q_5', tensor(0.9056)),\n",
       "   ('q_3', tensor(0.9051)),\n",
       "   ('q_1', tensor(0.9024)),\n",
       "   ('q_0', tensor(0.9014))]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run query matcher\n",
    "\n",
    "res_1=query_matcher.match_queries(raw_text_list,prep_type='concat')\n",
    "res_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb1b35",
   "metadata": {},
   "source": [
    "##### 2.4 Back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aefdc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym insertation (pairs) based on wordnet (2 new sentences for each one)\n",
    "\n",
    "raw_sent_pairs_transaug=data_prepper.aug_trans_swap(raw_sent_pairs,from_lang='en',to_lang='de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f00b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym insertation (not pairs) based on wordnet (2 new sentences for each one)\n",
    "\n",
    "raw_sent_notpairs_transaug=data_prepper.aug_trans_swap(raw_sent_notpairs,from_lang='en',to_lang='de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8da6416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101,  103, 2655,  ...,    0,    0,    0],\n",
       "        [ 101,  103, 1005,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2043, 1045,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1005,  ...,    0,    0,    0],\n",
       "        [ 101, 2016, 8916,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]), 'labels': tensor([[ 101, 7592, 1010,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 2655,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1005,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2043, 1045,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 1005,  ...,    0,    0,    0],\n",
       "        [ 101, 2016, 8916,  ...,    0,    0,    0]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat dataset for BERT with language back translation data augmentation\n",
    "\n",
    "BERT_tuner=BERTtuner()\n",
    "BERT_tuner.load_from_web(model_name='bert-base-uncased')\n",
    "\n",
    "bert_inputs_transaug=BERT_tuner.prepare_data_for_BERT_train(raw_sent_pairs_transaug,raw_sent_notpairs_transaug)\n",
    "bert_inputs_transaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34b3822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:47<00:00, 23.64s/it, loss=14.9]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:53<00:00, 26.67s/it, loss=9.25]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:55<00:00, 27.93s/it, loss=6.79]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 2/2 [01:01<00:00, 30.72s/it, loss=5.81]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:53<00:00, 26.66s/it, loss=4.82]\n"
     ]
    }
   ],
   "source": [
    "# Run bechmark BERT\n",
    "\n",
    "m3,loss3=BERT_tuner.train_BERT(bert_inputs_transaug,epochs=5,batch_size=16,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08f490b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gedas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2064, 11498,  ...,     0,     0,     0],\n",
       "        [  101,  2054,  2024,  ...,     0,     0,     0],\n",
       "        [  101,  2054,  2003,  ...,     0,     0,     0],\n",
       "        [  101,  2054, 11498,  ...,     0,     0,     0],\n",
       "        [  101,  2043,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  2339,  2003,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'embeddings': tensor([[ 0.0751,  0.0975,  0.0400,  ..., -0.1474,  0.0068, -0.0318],\n",
       "        [ 0.0773,  0.1062,  0.0407,  ..., -0.1609,  0.0088, -0.0353],\n",
       "        [ 0.0545,  0.0808,  0.0276,  ..., -0.1230,  0.0042, -0.0322],\n",
       "        [ 0.0652,  0.0887,  0.0345,  ..., -0.1356,  0.0069, -0.0276],\n",
       "        [ 0.0592,  0.0816,  0.0299,  ..., -0.1227,  0.0035, -0.0330],\n",
       "        [ 0.0449,  0.0666,  0.0229,  ..., -0.0989,  0.0055, -0.0226]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare question embeddings using newly trained model (query question)\n",
    "\n",
    "query_matcher=QueryMatcher(m3,BERT_tuner.model_tokenizer,False,True) # Init query matcher class\n",
    "question_embeddings=query_matcher.calc_question_embeddings(questions_list,save=False) # Calcuate embeddings\n",
    "question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99b9bb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_0': {'0_sentences': [('q_2', tensor(0.9991)),\n",
       "   ('q_5', tensor(0.9989)),\n",
       "   ('q_4', tensor(0.9988)),\n",
       "   ('q_3', tensor(0.9986)),\n",
       "   ('q_1', tensor(0.9981)),\n",
       "   ('q_0', tensor(0.9978))],\n",
       "  '1_sentences': [('q_4', tensor(0.9983)),\n",
       "   ('q_2', tensor(0.9982)),\n",
       "   ('q_3', tensor(0.9981)),\n",
       "   ('q_1', tensor(0.9980)),\n",
       "   ('q_5', tensor(0.9979)),\n",
       "   ('q_0', tensor(0.9977))],\n",
       "  '2_sentences': [('q_4', tensor(0.9953)),\n",
       "   ('q_2', tensor(0.9950)),\n",
       "   ('q_3', tensor(0.9948)),\n",
       "   ('q_1', tensor(0.9947)),\n",
       "   ('q_5', tensor(0.9947)),\n",
       "   ('q_0', tensor(0.9944))],\n",
       "  '3_sentences': [('q_4', tensor(0.9942)),\n",
       "   ('q_2', tensor(0.9940)),\n",
       "   ('q_3', tensor(0.9938)),\n",
       "   ('q_1', tensor(0.9938)),\n",
       "   ('q_5', tensor(0.9936)),\n",
       "   ('q_0', tensor(0.9934))],\n",
       "  '4_sentences': [('q_4', tensor(0.9904)),\n",
       "   ('q_2', tensor(0.9902)),\n",
       "   ('q_3', tensor(0.9898)),\n",
       "   ('q_5', tensor(0.9898)),\n",
       "   ('q_1', tensor(0.9897)),\n",
       "   ('q_0', tensor(0.9893))],\n",
       "  '5_sentences': [('q_4', tensor(0.9831)),\n",
       "   ('q_2', tensor(0.9829)),\n",
       "   ('q_5', tensor(0.9822)),\n",
       "   ('q_3', tensor(0.9820)),\n",
       "   ('q_1', tensor(0.9816)),\n",
       "   ('q_0', tensor(0.9810))],\n",
       "  '6_sentences': [('q_4', tensor(0.9752)),\n",
       "   ('q_2', tensor(0.9750)),\n",
       "   ('q_5', tensor(0.9745)),\n",
       "   ('q_3', tensor(0.9743)),\n",
       "   ('q_1', tensor(0.9738)),\n",
       "   ('q_0', tensor(0.9732))],\n",
       "  '7_sentences': [('q_4', tensor(0.9654)),\n",
       "   ('q_2', tensor(0.9653)),\n",
       "   ('q_5', tensor(0.9646)),\n",
       "   ('q_3', tensor(0.9643)),\n",
       "   ('q_1', tensor(0.9636)),\n",
       "   ('q_0', tensor(0.9629))],\n",
       "  '8_sentences': [('q_2', tensor(0.9557)),\n",
       "   ('q_4', tensor(0.9556)),\n",
       "   ('q_5', tensor(0.9548)),\n",
       "   ('q_3', tensor(0.9544)),\n",
       "   ('q_1', tensor(0.9534)),\n",
       "   ('q_0', tensor(0.9526))],\n",
       "  '9_sentences': [('q_2', tensor(0.9199)),\n",
       "   ('q_4', tensor(0.9191)),\n",
       "   ('q_5', tensor(0.9180)),\n",
       "   ('q_3', tensor(0.9175)),\n",
       "   ('q_1', tensor(0.9157)),\n",
       "   ('q_0', tensor(0.9147))],\n",
       "  '10_sentences': [('q_2', tensor(0.9016)),\n",
       "   ('q_4', tensor(0.9005)),\n",
       "   ('q_5', tensor(0.8992)),\n",
       "   ('q_3', tensor(0.8988)),\n",
       "   ('q_1', tensor(0.8966)),\n",
       "   ('q_0', tensor(0.8955))]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run query matcher\n",
    "\n",
    "res_1=query_matcher.match_queries(raw_text_list,prep_type='concat')\n",
    "res_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06edf8e2",
   "metadata": {},
   "source": [
    "##### 2.5 Back translation + synonym insertation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0bd14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate syn aug and trans aug\n",
    "\n",
    "ran_sent_pairs_combaug=raw_sent_pairs_transaug+raw_sent_pairs_synaug # Combine pair data for synonym augmentation and translation augmentaion\n",
    "raw_sent_notpairs_combaug=raw_sent_notpairs_transaug+raw_sent_notpairs_synaug # Combine not pair data for synonym augmentation and translation augmentaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aee17c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  1010,  ...,     0,     0,     0],\n",
       "        [  101,  1045,  2655,  ...,     0,     0,     0],\n",
       "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2019,  2063,  ...,     0,     0,     0],\n",
       "        [  101,   103, 20565,  ...,     0,     0,     0],\n",
       "        [  101,  2026,   103,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]), 'labels': tensor([[  101,  7592,  1010,  ...,     0,     0,     0],\n",
       "        [  101,  1045,  2655,  ...,     0,     0,     0],\n",
       "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2019,  2063,  ...,     0,     0,     0],\n",
       "        [  101,  2026, 20565,  ...,     0,     0,     0],\n",
       "        [  101,  2026,  3566,  ...,     0,     0,     0]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat dataset for BERT with language back translation data augmentation combined with syn insertation augmentaiton\n",
    "\n",
    "BERT_tuner=BERTtuner()\n",
    "BERT_tuner.load_from_web(model_name='bert-base-uncased')\n",
    "\n",
    "bert_inputs_combaug=BERT_tuner.prepare_data_for_BERT_train(ran_sent_pairs_combaug,raw_sent_notpairs_combaug)\n",
    "bert_inputs_combaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "254adb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:47<00:00, 41.81s/it, loss=9.52]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:52<00:00, 43.11s/it, loss=5.98]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:48<00:00, 42.19s/it, loss=3.12]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:45<00:00, 41.45s/it, loss=1.09]\n",
      "Epoch 4: 100%|███████████████████████████████████████████████████████████████| 4/4 [02:57<00:00, 44.50s/it, loss=0.501]\n"
     ]
    }
   ],
   "source": [
    "# Run bechmark BERT\n",
    "\n",
    "m4,loss4=BERT_tuner.train_BERT(bert_inputs_combaug,epochs=5,batch_size=16,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72837789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gedas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2064, 11498,  ...,     0,     0,     0],\n",
       "        [  101,  2054,  2024,  ...,     0,     0,     0],\n",
       "        [  101,  2054,  2003,  ...,     0,     0,     0],\n",
       "        [  101,  2054, 11498,  ...,     0,     0,     0],\n",
       "        [  101,  2043,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  2339,  2003,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'embeddings': tensor([[ 0.0539,  0.0405,  0.0050,  ..., -0.1086,  0.0415, -0.0420],\n",
       "        [ 0.0554,  0.0442,  0.0048,  ..., -0.1174,  0.0462, -0.0456],\n",
       "        [ 0.0397,  0.0331,  0.0010,  ..., -0.0901,  0.0314, -0.0386],\n",
       "        [ 0.0473,  0.0368,  0.0035,  ..., -0.0984,  0.0381, -0.0372],\n",
       "        [ 0.0438,  0.0327,  0.0021,  ..., -0.0900,  0.0311, -0.0395],\n",
       "        [ 0.0318,  0.0274,  0.0021,  ..., -0.0732,  0.0263, -0.0279]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare question embeddings using newly trained model (query question)\n",
    "\n",
    "query_matcher=QueryMatcher(m4,BERT_tuner.model_tokenizer,False,True) # Init query matcher class\n",
    "question_embeddings=query_matcher.calc_question_embeddings(questions_list,save=False) # Calcuate embeddings\n",
    "question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "949ff250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_0': {'0_sentences': [('q_2', tensor(0.9995)),\n",
       "   ('q_5', tensor(0.9994)),\n",
       "   ('q_4', tensor(0.9993)),\n",
       "   ('q_3', tensor(0.9992)),\n",
       "   ('q_1', tensor(0.9990)),\n",
       "   ('q_0', tensor(0.9988))],\n",
       "  '1_sentences': [('q_3', tensor(0.9991)),\n",
       "   ('q_1', tensor(0.9991)),\n",
       "   ('q_2', tensor(0.9991)),\n",
       "   ('q_4', tensor(0.9990)),\n",
       "   ('q_0', tensor(0.9989)),\n",
       "   ('q_5', tensor(0.9989))],\n",
       "  '2_sentences': [('q_1', tensor(0.9976)),\n",
       "   ('q_3', tensor(0.9976)),\n",
       "   ('q_0', tensor(0.9975)),\n",
       "   ('q_4', tensor(0.9975)),\n",
       "   ('q_2', tensor(0.9975)),\n",
       "   ('q_5', tensor(0.9972))],\n",
       "  '3_sentences': [('q_1', tensor(0.9971)),\n",
       "   ('q_3', tensor(0.9970)),\n",
       "   ('q_0', tensor(0.9969)),\n",
       "   ('q_4', tensor(0.9969)),\n",
       "   ('q_2', tensor(0.9969)),\n",
       "   ('q_5', tensor(0.9966))],\n",
       "  '4_sentences': [('q_1', tensor(0.9946)),\n",
       "   ('q_3', tensor(0.9945)),\n",
       "   ('q_0', tensor(0.9944)),\n",
       "   ('q_2', tensor(0.9943)),\n",
       "   ('q_4', tensor(0.9942)),\n",
       "   ('q_5', tensor(0.9941))],\n",
       "  '5_sentences': [('q_1', tensor(0.9907)),\n",
       "   ('q_3', tensor(0.9907)),\n",
       "   ('q_2', tensor(0.9906)),\n",
       "   ('q_0', tensor(0.9905)),\n",
       "   ('q_4', tensor(0.9905)),\n",
       "   ('q_5', tensor(0.9904))],\n",
       "  '6_sentences': [('q_1', tensor(0.9878)),\n",
       "   ('q_3', tensor(0.9878)),\n",
       "   ('q_0', tensor(0.9876)),\n",
       "   ('q_4', tensor(0.9874)),\n",
       "   ('q_2', tensor(0.9874)),\n",
       "   ('q_5', tensor(0.9874))],\n",
       "  '7_sentences': [('q_1', tensor(0.9836)),\n",
       "   ('q_3', tensor(0.9835)),\n",
       "   ('q_0', tensor(0.9833)),\n",
       "   ('q_4', tensor(0.9832)),\n",
       "   ('q_5', tensor(0.9831)),\n",
       "   ('q_2', tensor(0.9831))],\n",
       "  '8_sentences': [('q_3', tensor(0.9792)),\n",
       "   ('q_1', tensor(0.9792)),\n",
       "   ('q_4', tensor(0.9789)),\n",
       "   ('q_5', tensor(0.9789)),\n",
       "   ('q_0', tensor(0.9789)),\n",
       "   ('q_2', tensor(0.9788))],\n",
       "  '9_sentences': [('q_3', tensor(0.9646)),\n",
       "   ('q_1', tensor(0.9645)),\n",
       "   ('q_4', tensor(0.9642)),\n",
       "   ('q_2', tensor(0.9641)),\n",
       "   ('q_0', tensor(0.9641)),\n",
       "   ('q_5', tensor(0.9640))],\n",
       "  '10_sentences': [('q_3', tensor(0.9559)),\n",
       "   ('q_1', tensor(0.9558)),\n",
       "   ('q_4', tensor(0.9555)),\n",
       "   ('q_2', tensor(0.9554)),\n",
       "   ('q_0', tensor(0.9554)),\n",
       "   ('q_5', tensor(0.9553))]}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run query matcher\n",
    "\n",
    "res_1=query_matcher.match_queries(raw_text_list,prep_type='concat')\n",
    "res_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee82754",
   "metadata": {},
   "source": [
    "### 3. BERT optimization via Bayes opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89783dee",
   "metadata": {},
   "source": [
    "##### 0. Read labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f391bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gedas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Read labelled questions\n",
    "\n",
    "data_prepper=DataPrepper()\n",
    "target_data_df=data_prepper.prepare_corpus_labelled(CORPUS_LABELLED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba4749",
   "metadata": {},
   "source": [
    "##### 3.1 Prepare loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b73c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate top n precision (common metric in document retrieval systems)\n",
    "\n",
    "def calculate_topn_precision(target_data:pd.DataFrame,predicted_data:pd.DataFrame,top_n:int=3):\n",
    "    \n",
    "    #1 Reset index\n",
    "    predicted_data.reset_index(inplace=True)\n",
    "    \n",
    "    #2. Type conversion for merging\n",
    "    predicted_data['question_id']=predicted_data['question_id'].astype(int)\n",
    "    target_data['relevant_q_id']=target_data['relevant_q_id'].astype(int)\n",
    "\n",
    "    #3. Merging\n",
    "    t_p=predicted_data.merge(target_data,left_on=['question_file','question_id'],right_on=['question_file','relevant_q_id'],how='outer')\n",
    "    \n",
    "    #4. FFill log file id\n",
    "    t_p.loc[:,'log_file'].ffill(inplace=True)\n",
    "    \n",
    "    #4. Labelling\n",
    "    t_p['question_true_label']=np.where(t_p['question_id']==t_p['relevant_q_id'],1,0)\n",
    "    \n",
    "    #5. Metric function\n",
    "    top_n=t_p.sort_values(['log_file','question_similarity_score'],ascending=False).groupby('log_file').head(top_n)\n",
    "    precision=top_n[top_n['question_true_label']==1].shape[0]/top_n.shape[0]\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee770e5d",
   "metadata": {},
   "source": [
    "##### 3.2 Prepare objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9ec7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes optimization function\n",
    "\n",
    "def bayesopt_obj_function(epochs=5,batch_size=16,learning_rate=1e-4,model_type='bert-base-uncased',aggregation_type='average'):\n",
    "    \n",
    "    #1. Initialize variables\n",
    "    epochs=int(round(epochs))\n",
    "    batch_size=int(round(batch_size))\n",
    "    \n",
    "    #2. Train and predict\n",
    "    #2.1 Prepare data (use combined augmentation as gave smallest training loss)\n",
    "    BERT_tuner=BERTtuner()\n",
    "    BERT_tuner.load_from_web(model_name=model_type)\n",
    "    bert_inputs_combaug=BERT_tuner.prepare_data_for_BERT_train(ran_sent_pairs_combaug,raw_sent_notpairs_combaug)\n",
    "    \n",
    "    #2.2 Train bert\n",
    "    m,loss=BERT_tuner.train_BERT(bert_inputs_combaug,epochs=epochs,batch_size=batch_size,learning_rate=learning_rate)\n",
    "    \n",
    "    #2.3 Get new question embeddings\n",
    "    query_matcher=QueryMatcher(m,BERT_tuner.model_tokenizer,False,True) # Init query matcher class\n",
    "    question_embeddings=query_matcher.calc_question_embeddings(questions_list,save=False) # Calcuate embeddings\n",
    "    \n",
    "    #2.4 Match queries to questions (calculate queries embeddings)\n",
    "    queries_matched_dict=query_matcher.match_queries(raw_text_list,prep_type='concat')\n",
    "    \n",
    "    #2.5 Convert result dict to df and aggregate results\n",
    "    queries_matched_dict_df=query_matcher.match_queries_to_df(queries_matched_dict,aggregation_type=aggregation_type)\n",
    "    \n",
    "    #2.6 Calculate top k precision\n",
    "    top_n_prec=calculate_topn_precision(target_data_df,queries_matched_dict_df,top_n=3)\n",
    "    \n",
    "    return top_n_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb72ba4",
   "metadata": {},
   "source": [
    "##### 3.3 Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "faa12b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameter dict\n",
    "\n",
    "param_dict={'epochs':(2,10),'batch_size':(8,32),'learning_rate':(1e-6,1e-1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca6f1715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... |  epochs   | learni... |\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 2/2 [03:11<00:00, 95.77s/it, loss=52.9]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████| 2/2 [05:50<00:00, 175.47s/it, loss=30.9]\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████| 2/2 [14:00<00:00, 420.03s/it, loss=40.1]\n",
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████| 2/2 [03:50<00:00, 115.47s/it, loss=39.7]\n",
      "Epoch 4: 100%|███████████████████████████████████████████████████████████████| 2/2 [09:03<00:00, 271.67s/it, loss=25.3]\n",
      "Epoch 5: 100%|███████████████████████████████████████████████████████████████| 2/2 [05:59<00:00, 179.69s/it, loss=43.5]\n",
      "Epoch 6: 100%|███████████████████████████████████████████████████████████████| 2/2 [08:19<00:00, 249.52s/it, loss=36.1]\n",
      "Epoch 7: 100%|███████████████████████████████████████████████████████████████| 2/2 [07:33<00:00, 226.82s/it, loss=42.3]\n",
      "Epoch 8: 100%|███████████████████████████████████████████████████████████████| 2/2 [06:47<00:00, 203.91s/it, loss=55.7]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.3333  \u001b[0m | \u001b[0m 29.55   \u001b[0m | \u001b[0m 9.388   \u001b[0m | \u001b[0m 0.09017 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 2/2 [03:08<00:00, 94.20s/it, loss=49.9]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 2/2 [03:18<00:00, 99.46s/it, loss=45.8]\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████| 2/2 [12:51<00:00, 386.00s/it, loss=30.6]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 31.21   \u001b[0m | \u001b[95m 3.436   \u001b[0m | \u001b[95m 0.08291 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 3/3 [03:03<00:00, 61.10s/it, loss=23.3]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████| 3/3 [06:59<00:00, 139.92s/it, loss=11.9]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 3/3 [03:29<00:00, 69.95s/it, loss=15.5]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.6667  \u001b[0m | \u001b[95m 25.83   \u001b[0m | \u001b[95m 2.569   \u001b[0m | \u001b[95m 0.03509 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 5/5 [03:01<00:00, 36.34s/it, loss=8.94]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████| 5/5 [11:49<00:00, 141.93s/it, loss=7.03]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 5/5 [03:28<00:00, 41.71s/it, loss=13.9]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 5/5 [03:29<00:00, 41.83s/it, loss=8.21]\n",
      "Epoch 4: 100%|██████████████████████████████████████████████████████████████████| 5/5 [03:32<00:00, 42.42s/it, loss=12]\n",
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████| 5/5 [03:33<00:00, 42.68s/it, loss=11.7]\n",
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████| 5/5 [03:34<00:00, 42.98s/it, loss=9.94]\n",
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████| 5/5 [03:22<00:00, 40.45s/it, loss=10.4]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 14.08   \u001b[0m | \u001b[0m 7.877   \u001b[0m | \u001b[0m 0.02062 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:51<00:00, 42.94s/it, loss=34.2]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████| 4/4 [08:11<00:00, 122.89s/it, loss=33.1]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:21<00:00, 50.28s/it, loss=23.5]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:24<00:00, 51.23s/it, loss=31.5]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:16<00:00, 49.15s/it, loss=20.5]\n",
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:20<00:00, 50.15s/it, loss=20.1]\n",
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:31<00:00, 52.87s/it, loss=26.8]\n",
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:36<00:00, 54.22s/it, loss=15.7]\n",
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:29<00:00, 52.30s/it, loss=12.7]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.75    \u001b[0m | \u001b[95m 16.58   \u001b[0m | \u001b[95m 9.223   \u001b[0m | \u001b[95m 0.05042 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:41<00:00, 40.34s/it, loss=9.77]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:48<00:00, 42.20s/it, loss=4.54]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:46<00:00, 41.66s/it, loss=2.05]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:56<00:00, 44.11s/it, loss=1.79]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:09<00:00, 47.39s/it, loss=1.61]\n",
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:42<00:00, 55.63s/it, loss=1.64]\n",
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████| 4/4 [04:05<00:00, 61.37s/it, loss=1.44]\n",
      "Epoch 7: 100%|█████████████████████████████████████████████████████████████████| 4/4 [04:09<00:00, 62.47s/it, loss=1.7]\n",
      "Epoch 8: 100%|████████████████████████████████████████████████████████████████| 4/4 [04:21<00:00, 65.49s/it, loss=1.52]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 14.55   \u001b[0m | \u001b[95m 9.021   \u001b[0m | \u001b[95m 0.006341\u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████| 3/3 [03:04<00:00, 61.36s/it, loss=145]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████| 3/3 [10:49<00:00, 216.39s/it, loss=88.8]\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████| 3/3 [06:33<00:00, 131.12s/it, loss=41.6]\n",
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████| 3/3 [18:15<00:00, 365.29s/it, loss=51.7]\n",
      "Epoch 4: 100%|███████████████████████████████████████████████████████████████| 3/3 [07:38<00:00, 152.94s/it, loss=53.2]\n",
      "Epoch 5: 100%|███████████████████████████████████████████████████████████████| 3/3 [15:10<00:00, 303.62s/it, loss=34.2]\n",
      "Epoch 6: 100%|███████████████████████████████████████████████████████████████| 3/3 [16:20<00:00, 326.86s/it, loss=42.1]\n",
      "Epoch 7: 100%|███████████████████████████████████████████████████████████████| 3/3 [11:31<00:00, 230.56s/it, loss=40.7]\n",
      "Epoch 8: 100%|███████████████████████████████████████████████████████████████| 3/3 [14:34<00:00, 291.52s/it, loss=27.2]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 26.46   \u001b[0m | \u001b[0m 9.367   \u001b[0m | \u001b[0m 0.09626 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████| 5/5 [08:51<00:00, 106.34s/it, loss=17.2]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████| 5/5 [09:27<00:00, 113.41s/it, loss=15.2]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 5/5 [05:10<00:00, 62.17s/it, loss=13.1]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 5/5 [04:47<00:00, 57.50s/it, loss=18.1]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████| 5/5 [05:02<00:00, 60.53s/it, loss=17.3]\n",
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████| 5/5 [04:30<00:00, 54.17s/it, loss=8.96]\n",
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████| 5/5 [04:07<00:00, 49.42s/it, loss=12.4]\n",
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████| 5/5 [04:13<00:00, 50.75s/it, loss=10.3]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6667  \u001b[0m | \u001b[0m 13.59   \u001b[0m | \u001b[0m 8.338   \u001b[0m | \u001b[0m 0.02299 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:01<00:00, 45.41s/it, loss=75.9]\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████| 4/4 [22:58<00:00, 344.56s/it, loss=41]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:45<00:00, 56.27s/it, loss=46.4]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 4/4 [05:08<00:00, 77.22s/it, loss=22.1]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:53<00:00, 58.28s/it, loss=22.5]\n",
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:42<00:00, 55.73s/it, loss=17.9]\n",
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:40<00:00, 55.03s/it, loss=19.9]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 16.82   \u001b[0m | \u001b[0m 6.893   \u001b[0m | \u001b[0m 0.04229 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 6/6 [06:42<00:00, 67.02s/it, loss=24.2]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 6/6 [09:17<00:00, 92.89s/it, loss=37.3]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 6/6 [02:45<00:00, 27.63s/it, loss=26.9]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6667  \u001b[0m | \u001b[0m 11.38   \u001b[0m | \u001b[0m 3.37    \u001b[0m | \u001b[0m 0.03928 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████| 4/4 [08:00<00:00, 120.14s/it, loss=34.7]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:26<00:00, 51.68s/it, loss=35.4]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:33<00:00, 53.44s/it, loss=23.7]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:42<00:00, 55.70s/it, loss=22.5]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:36<00:00, 54.14s/it, loss=33.9]\n",
      "Epoch 5: 100%|██████████████████████████████████████████████████████████████████| 4/4 [03:56<00:00, 59.01s/it, loss=21]\n",
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:51<00:00, 57.87s/it, loss=26.9]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 16.87   \u001b[0m | \u001b[0m 6.918   \u001b[0m | \u001b[0m 0.03972 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 3/3 [03:29<00:00, 69.71s/it, loss=57.6]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████| 3/3 [18:36<00:00, 372.30s/it, loss=21.8]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 3/3 [04:23<00:00, 87.80s/it, loss=41.8]\n",
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████| 3/3 [13:53<00:00, 277.74s/it, loss=38.3]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.75    \u001b[0m | \u001b[0m 19.62   \u001b[0m | \u001b[0m 4.057   \u001b[0m | \u001b[0m 0.07224 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 6/6 [09:47<00:00, 97.85s/it, loss=30.8]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 6/6 [03:00<00:00, 30.12s/it, loss=20.4]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 6/6 [02:47<00:00, 27.98s/it, loss=20.4]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 6/6 [02:46<00:00, 27.71s/it, loss=29.1]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 10.12   \u001b[0m | \u001b[0m 4.047   \u001b[0m | \u001b[0m 0.04269 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████| 5/5 [09:58<00:00, 119.67s/it, loss=13]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 5/5 [03:04<00:00, 36.83s/it, loss=52.4]\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████| 5/5 [15:05<00:00, 181.18s/it, loss=21.2]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.3333  \u001b[0m | \u001b[0m 13.29   \u001b[0m | \u001b[0m 2.854   \u001b[0m | \u001b[0m 0.05531 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 2/2 [03:15<00:00, 97.53s/it, loss=24.4]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████| 2/2 [17:46<00:00, 533.38s/it, loss=39.9]\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████| 2/2 [05:56<00:00, 178.31s/it, loss=24.8]\n",
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████| 2/2 [04:01<00:00, 120.63s/it, loss=54.3]\n",
      "Epoch 4: 100%|███████████████████████████████████████████████████████████████| 2/2 [05:40<00:00, 170.14s/it, loss=22.9]\n",
      "Epoch 5: 100%|█████████████████████████████████████████████████████████████████| 2/2 [04:02<00:00, 121.31s/it, loss=34]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6     \u001b[0m | \u001b[0m 30.1    \u001b[0m | \u001b[0m 6.225   \u001b[0m | \u001b[0m 0.05285 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████| 3/3 [03:22<00:00, 67.39s/it, loss=38]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████| 3/3 [12:51<00:00, 257.08s/it, loss=18.5]\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████| 3/3 [12:48<00:00, 256.02s/it, loss=24.3]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 3/3 [03:44<00:00, 74.87s/it, loss=24.7]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████| 3/3 [03:43<00:00, 74.62s/it, loss=16.2]\n",
      "Epoch 5: 100%|██████████████████████████████████████████████████████████████████| 3/3 [03:36<00:00, 72.19s/it, loss=17]\n",
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████| 3/3 [03:41<00:00, 73.91s/it, loss=13.2]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 23.03   \u001b[0m | \u001b[0m 6.728   \u001b[0m | \u001b[0m 0.03767 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 3/3 [03:44<00:00, 74.89s/it, loss=29.5]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████| 3/3 [15:02<00:00, 300.73s/it, loss=19.6]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 19.65   \u001b[0m | \u001b[0m 2.285   \u001b[0m | \u001b[0m 0.03762 \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gedas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]C:\\Users\\gedas\\OneDrive\\Desktop\\job_search\\Biomapas\\data_analysis\\BERTtuner.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 3/3 [03:34<00:00, 71.59s/it, loss=52.4]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 3/3 [04:05<00:00, 81.84s/it, loss=75.5]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 3/3 [03:43<00:00, 74.44s/it, loss=49.2]\n",
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████| 3/3 [16:14<00:00, 324.91s/it, loss=58]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████| 3/3 [04:35<00:00, 91.94s/it, loss=56.9]\n",
      "Epoch 5: 100%|███████████████████████████████████████████████████████████████| 3/3 [08:46<00:00, 175.51s/it, loss=43.5]\n",
      "Epoch 6: 100%|███████████████████████████████████████████████████████████████| 3/3 [06:03<00:00, 121.05s/it, loss=36.1]\n",
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████| 3/3 [04:54<00:00, 98.21s/it, loss=32.3]\n",
      "Epoch 8: 100%|███████████████████████████████████████████████████████████████| 3/3 [06:22<00:00, 127.54s/it, loss=31.8]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.6667  \u001b[0m | \u001b[0m 25.29   \u001b[0m | \u001b[0m 8.747   \u001b[0m | \u001b[0m 0.07063 \u001b[0m |\n",
      "=============================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Final Results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'target': 0.3333333333333333,\n",
       "  'params': {'batch_size': 29.553730742713505,\n",
       "   'epochs': 9.38814220981831,\n",
       "   'learning_rate': 0.09016813746748065}},\n",
       " {'target': 0.5,\n",
       "  'params': {'batch_size': 31.21126164807619,\n",
       "   'epochs': 3.4360530485231484,\n",
       "   'learning_rate': 0.08290607763622375}},\n",
       " {'target': 0.6666666666666666,\n",
       "  'params': {'batch_size': 25.82886959124032,\n",
       "   'epochs': 2.568654217598951,\n",
       "   'learning_rate': 0.03509096549672737}},\n",
       " {'target': 0.5,\n",
       "  'params': {'batch_size': 14.082918238288205,\n",
       "   'epochs': 7.8770800943546,\n",
       "   'learning_rate': 0.02062272812911489}},\n",
       " {'target': 0.75,\n",
       "  'params': {'batch_size': 16.58486990317661,\n",
       "   'epochs': 9.22325344967566,\n",
       "   'learning_rate': 0.050416789012716325}},\n",
       " {'target': 1.0,\n",
       "  'params': {'batch_size': 14.546609252900712,\n",
       "   'epochs': 9.020573691174002,\n",
       "   'learning_rate': 0.00634060305118127}},\n",
       " {'target': 0.5,\n",
       "  'params': {'batch_size': 26.46036422764427,\n",
       "   'epochs': 9.366552274643832,\n",
       "   'learning_rate': 0.09626301422230656}},\n",
       " {'target': 0.6666666666666666,\n",
       "  'params': {'batch_size': 13.594105785806205,\n",
       "   'epochs': 8.33777216904106,\n",
       "   'learning_rate': 0.02299217870094651}},\n",
       " {'target': 1.0,\n",
       "  'params': {'batch_size': 16.815469147236627,\n",
       "   'epochs': 6.89347864291707,\n",
       "   'learning_rate': 0.042294711764074164}},\n",
       " {'target': 0.6666666666666666,\n",
       "  'params': {'batch_size': 11.37696325028909,\n",
       "   'epochs': 3.36979317648688,\n",
       "   'learning_rate': 0.03927701886037619}},\n",
       " {'target': 0.5,\n",
       "  'params': {'batch_size': 16.873812834446788,\n",
       "   'epochs': 6.918301677133819,\n",
       "   'learning_rate': 0.039716977950455395}},\n",
       " {'target': 0.75,\n",
       "  'params': {'batch_size': 19.616581203166938,\n",
       "   'epochs': 4.057035394541216,\n",
       "   'learning_rate': 0.0722384563232527}},\n",
       " {'target': 0.5,\n",
       "  'params': {'batch_size': 10.121895746450386,\n",
       "   'epochs': 4.047046014178011,\n",
       "   'learning_rate': 0.042686057889025755}},\n",
       " {'target': 0.3333333333333333,\n",
       "  'params': {'batch_size': 13.287376126899865,\n",
       "   'epochs': 2.8536782649696866,\n",
       "   'learning_rate': 0.05531478537878845}},\n",
       " {'target': 0.6,\n",
       "  'params': {'batch_size': 30.097485567769397,\n",
       "   'epochs': 6.225365729044798,\n",
       "   'learning_rate': 0.0528495183649729}},\n",
       " {'target': 0.5,\n",
       "  'params': {'batch_size': 23.02543594630586,\n",
       "   'epochs': 6.728393520623053,\n",
       "   'learning_rate': 0.03767321007784707}},\n",
       " {'target': 0.5,\n",
       "  'params': {'batch_size': 19.65343650630858,\n",
       "   'epochs': 2.285269522496926,\n",
       "   'learning_rate': 0.03761805849371312}},\n",
       " {'target': 0.6666666666666666,\n",
       "  'params': {'batch_size': 25.29237086463581,\n",
       "   'epochs': 8.746868371693541,\n",
       "   'learning_rate': 0.07062709473933608}}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize\n",
    "\n",
    "opt=BayesianOptimization(bayesopt_obj_function,param_dict,verbose=2) # Creates bayesian opt function\n",
    "opt.maximize(init_points=3,n_iter=15,acq='ei') # Maximize utility function\n",
    "\n",
    "print('-' * 100)\n",
    "print('Final Results')\n",
    "opt.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b3548c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 1.0,\n",
       " 'params': {'batch_size': 14.546609252900712,\n",
       "  'epochs': 9.020573691174002,\n",
       "  'learning_rate': 0.00634060305118127}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c31ffd",
   "metadata": {},
   "source": [
    "### 4. Train and save best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c2223",
   "metadata": {},
   "source": [
    "##### 4.1 Train best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bert-base-uncased had been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:35<00:00, 38.75s/it, loss=19.6]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:46<00:00, 41.65s/it, loss=1.76]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:47<00:00, 41.97s/it, loss=1.64]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████| 4/4 [02:56<00:00, 44.01s/it, loss=1.59]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:06<00:00, 46.54s/it, loss=1.47]\n",
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████| 4/4 [03:14<00:00, 48.66s/it, loss=1.46]\n",
      "Epoch 6:  50%|████████████████████████████████                                | 2/4 [01:48<01:48, 54.33s/it, loss=1.54]"
     ]
    }
   ],
   "source": [
    "# Train best model\n",
    "\n",
    "# Prepare data\n",
    "BERT_tuner=BERTtuner()\n",
    "BERT_tuner.load_from_web(model_name='bert-base-uncased')\n",
    "\n",
    "bert_inputs_combaug=BERT_tuner.prepare_data_for_BERT_train(ran_sent_pairs_combaug,raw_sent_notpairs_combaug)\n",
    "bert_inputs_combaug\n",
    "\n",
    "# Train\n",
    "m,loss=BERT_tuner.train_BERT(bert_inputs_combaug,epochs=9,batch_size=16,learning_rate=0.006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "\n",
    "query_matcher=QueryMatcher(m,BERT_tuner.model_tokenizer,False,True) # Init query matcher class\n",
    "question_embeddings=query_matcher.calc_question_embeddings(questions_list,save=True) # Calcuate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574b317",
   "metadata": {},
   "source": [
    "##### 4.2 Save best model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a35f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "torch.save(m, MODEL_PATH.joinpath('best_model'))\n",
    "torch.save(query_matcher.tokenizer, MODEL_PATH.joinpath('best_model_t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8975bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
